:PROPERTIES:
:ID:       0f8b7d24-d097-4785-89c7-ed550415f0a7
:END:
#+title: Transformer
#+STARTUP: latexpreview

Models handling sequential data were prominantly recurrent neural networks and convolutional neural networks, including a encoder-decoder. seq2seq passing attention information between encoder and decoder were performing better. Transformer completely removes RNN and CNN, instead built using attention mechanism only.

RNN's biggest disadvantage is the sequential nature. During training it needs to process one word at a time, increasing the time required greatly. Where as a convolution operation takes in all input vector at the same time. But then the disadvantage of CNN is that an input vector can only see closer neighbours ie, no long range dependencies. Transformer solves both of these issues.

* Necessary imports
#+begin_src python :tangle ~/projects/ultrasound/models/transformerv2.py :mkdirp yes
import torch
import torch.nn as nn
import torch.nn.functional as F
#+end_src
* Self-Attention
Fundamental operation of a transformer architecture. It is a seq to seq operation. Only operation that propogate information between the vectors. Every other operation is done on individual vectors of the input sequence. Operation is permutaion equivariant.
** Basic

#+ATTR_ORG: :width 800
[[./img/basicselfattn.png]]

\begin{equation*}
\{\vec{y_1},..\vec{y_j}..,\vec{y_t}\} = selfAttention(\{\vec{x_1},..\vec{x_i}..,\vec{x_t}\} )
\end{equation*}
Each of the vector in the sequence has a dimension of $k$.

\begin{equation*}
w_{ij}^{'} = x_i^Tx_j
\end{equation*}

\begin{equation*}
w_{ij} = \frac{e^{w_{ij}^{'}}}{ \sum_{j} e^{w_{ij}^{'}}}
\end{equation*}

Dot product's range is $(-\infty,\infty)$, softmax makes it to $[0,1]$

\begin{equation*}
\vec{y_i} = \sum_{j} w_{ij}\vec{x_j}
\end{equation*}

#+begin_src python :tangle ~/projects/ultrasound/models/transformerv2.py :mkdirp yes
def basic_selfattn(x):
   unnorm_w = torch.bmm(x, x.transpose(1,2))
   w = F.softmax(unnorm_w, dim=2)
   y = torch.bmm(w, x)
   return y
#+end_src

** Q,K,V

#+ATTR_ORG: :width 600
[[./img/selfattn.png]]


#+ATTR_ORG: :width 600
[[./img/multiheadselfattn.png]]

#+begin_src python :tangle ~/projects/ultrasound/models/transformerv2.py :mkdirp yes
class SelfAttention(nn.Module):
    def __init__(self, k, h, mask=False):
        super(SelfAttention, self).__init__()

        assert k % h == 0
        self.k = k
        self.h = h

        self.wq = nn.Linear(k,k, bias=False)
        self.wk = nn.Linear(k,k, bias=False)
        self.wv = nn.Linear(k,k, bias=False)

        self.unify_heads = nn.Linear(k,k)
    

    def forward(self, x):

        b,t,k = x.size()

        # calculate q,k and v
        query = self.wq(x)
        key = self.wk(x)
        value = self.wv(x)

        # headwise dimension
        s = k // self.h

        # convert to heads
        # b,t,k -> b,t,h,s

        query = query.view(b,t,self.h,s)
        key = key.view(b,t,self.h,s)
        value = value.view(b,t,self.h,s)

        # fold heads into batch axis
        # b,t,h,s -> b,h,t,s -> b*h,t,s
        query = query.transpose(1,2).contiguous().view(b*self.h,t,s)
        key = key.transpose(1,2).contiguous().view(b*self.h,t,s)
        value = value.transpose(1,2).contiguous().view(b*self.h,t,s)

        # self attention calculation
        w = torch.bmm(query, key.transpose(1,2))
        w /= (k**(1/2))

        # if mask:
        #     indices = torch.triu_indices(t,t, offset=1)
        #     w[:, indices[0], indices[1]] = float('-inf')
        
        w = F.softmax(w, dim=2)

        # apply attention score
        y = torch.bmm(w, value)
        # fold back
        y = y.view(b,self.h,t,s)
        # unify heads
        y = y.transpose(1,2).contiguous().view(b,t,s*self.h)
        y = self.unify_heads(y)
        return y
#+end_src

* Transformer Block
#+ATTR_ORG: :width 800 :height 600
[[./img/transformerblock.png]]


#+begin_src python :tangle ~/projects/ultrasound/models/transformerv2.py :mkdirp yes
class TransformerBlock(nn.Module):
    def __init__(self, k, h):
        super(TransformerBlock, self).__init__()

        self.attention = SelfAttention(k,h)

        self.layernorm1 = nn.LayerNorm(k)
        self.layernorm2 = nn.LayerNorm(k)


        self.ff = nn.Sequential(nn.Linear(k, 4*k),
                                nn.ReLU(),
                                nn.Linear(4*k, k))
        
    def forward(self, x):

        y = self.attention(x)
        x = self.layernorm1(x+y)

        y = self.ff(x)
        out = self.layernorm2(x+y)
        return out
#+end_src

* Transformer Stack
#+ATTR_ORG: :width 800 :height 600
[[./img/transformer.png]]

#+begin_src python :tangle ~/projects/ultrasound/models/transformerv2.py :mkdirp yes
class Transformer(nn.Module):
    def __init__(self, k, h, depth):
        super(Transformer, self).__init__()

        transformer_blocks = []
        for i in range(depth):
            transformer_blocks.append(TransformerBlock(k,h))

        self.transformer_stack = nn.Sequential(*transformer_blocks)
        
    def forward(self, x):
        y = self.transformer_stack(x)
        return y

if __name__ == "__main__":
    t = Transformer(512,8,6)
    x = torch.randn(2,4,512)
    o = t(x)
    print(o.size())
#+end_src

* [[id:2e2431dd-539d-45fa-892d-f97dd250a9c1][Encoder-Decoder Transformer]] 
* [[id:22879d3c-8d85-4e5b-82f8-4b1edb63f42b][Illustrated-transformer]] 
