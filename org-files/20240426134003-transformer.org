:PROPERTIES:
:ID:       0f8b7d24-d097-4785-89c7-ed550415f0a7
:END:
#+title: Transformer
#+STARTUP: latexpreview



Models handling sequential data were prominantly recurrent neural networks and convolutional neural networks, including a encoder-decoder. seq2seq passing attention information betweek encoder and decoder were performing better. This work completely removes RNN and CNN, instead built using attention mechanism only.

RNN's biggest disadvantage is the sequential nature. During training it needs to process one word at a time, increasing the time required greatly. 

* Model Architecture
** Necessary imports
#+begin_src python :tangle ~/projects/main/models/transformer.py :mkdirp yes
import torch
import torch.nn as nn
#+end_src
** Overview
Transformer has an encoder-decoder structure.

[[./img/transformer_architecture.png]]

The encoder maps an input sequence $\vec{x}= (x_1, x_2,..,x_n)$ to a representation $\vec{z} = (z_1,z_2,..,z_n)$.

Given this $\vec{z}$ the decoder then generates output $\vec{y}=(y_1,y_2,..,y_m)$ one element at a time. Transformer is *Auto-Regressive*, as in the decoder will take the previous output element as additional input for the next step.

#+begin_src python :tangle ~/projects/ultrasound/models/transformer.py :mkdirp yes
import torch.nn as nn

class Transformer(nn.Module):
    def __init__(self ):
        super(Transformer, self).__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()

    def forward(self, x):
        z = self.encoder(x)
        y = self.decoder(z)
        return y
#+end_src

** Encoder Stack
Stack is made of six encoder blocks. Each block is made of a multiheaded self-attention and feed forward neural network. Every block has a residual connection over itself with a layer normalization after it.

#+begin_src python :tangle ~/projects/main/models/transformer.py :mkdirp yes
class Encoder(nn.Module):
    def __init__(self, Nx=6):
        super(Encoder, self).__init__()
        self.stack = nn.ModuleList()
        for i in range(Nx):
            self.stack.append(EncoderBlock())        

    def forward(self, x):
        out = self.stack(x)
        return out
#+end_src
*** Encoder Block
**** Multi-head attention
***** Self-attention
#+begin_src python :tangle ~/projects/main/models/transformer.py :mkdirp yes
class SelfAttention(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(SelfAttention, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.W_Q = nn.Linear(self.input_dim, self.output_dim)
        self.W_K = nn.Linear(self.input_dim, self.output_dim)
        self.W_V = nn.Linear(self.input_dim, self.output_dim)
        
    def forward(self, x):
        Q = self.W_Q(x)
        K = self.W_K(x)
        V = self.W_V(x)
        print(Q.size())
        return Q

if __name__ == "__main__":
    d_model = 512
    num_heads = 8
    m = 128
    batch = 4
    x = torch.randn(batch, m, d_model)
    a = SelfAttention(d_model, d_model//num_heads)
    q = a(x)
#+end_src
**** Feed forward neural network

** Decoder Stack

* [[http://jalammar.github.io/illustrated-transformer/][Illustrated Transformer]]
** Overview
Transformer use [[id:f9a2bb31-aad5-452c-90f0-7b24c7cd1ac4][Attention]] to increase the speed with which models can be trained.

#+ATTR_ORG: :width 600
[[./img/transformer1.png]]

[[./img/transformer2.png]]

Self attention layer let the encoder look at the other elements of the input sequence.
Decoder has an extra attention layer letting it focus on relevant parts of the input similar to the attention in seq2seq models.

#+ATTR_ORG: :width 600
[[./img/transformer3.png]]

An encoder recieves a list of vectors where each vector corresponds to a word. Length of word embedding vector is usually 512.

The length of the list is a hyperparameter.

Self attention layer looks at all vectors at the same time, where as the feed forward NN is applied parallelly. It is the same NN applied on each vector seperately giving us parallelization. 

** Self Attention

#+ATTR_ORG: :width 600
[[./img/transformer4.png]]

Steps in calculating self-attention:

*** Computing Query, Key and Value
create three vectors from each of the encoderâ€™s input vectors (in this case, the embedding of each word). So for each word, we create a *Query* vector, a *Key* vector, and a *Value* vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.

   #+ATTR_ORG: :width 600
   [[./img/transformer5.png]]

  New 3 vectors has smaller dimension ie 64. It doesn't need to be smaller.

**** Matrix form
Practically we calculate for all word embeddings at the same time using a matrix

[[./img/transformer6.png]]




*** Calculate Score
for a word/vector we calculate a score w.r.t other elements of input list.

1. score of word m w.r.t word n =  $S_{mn} = q_m.k_n$
2. Divide by $\sqrt{d_k}$, ie dimension of key vector to get stable gradients.
3. Pass the value to softmax function.
4. Multiply each value vector using this value.
5. Sum up the weighted value vectors from 4 to obtain the output of the self-attention layer for the word #m
   
**** Matrix form
#+ATTR_ORG: :width 600
[[./img/transformer7.png]]


*** Multi-headed attention
A single z encoding can be dominated by the word itself, hence calculating multiple Z increase the representation subspaces.

#+ATTR_ORG: :width 600
[[./img/transformer8.png]]


#+ATTR_ORG: :width 600
[[./img/z1.png]]

This produces 8 Z matrices for a single word/vector. But the next layer of feed forward NN expects a single matrix. Solution is to multiply a weight matrix with the concatenated z matrices producing a single z matrix.

#+ATTR_ORG: :width 600 :height 600
[[./img/z2.png]]

*** Summary

#+ATTR_ORG: :width 600
[[./img/transformer9.png]]

** Position information

In order to teach the model the information of the order of the words we combine the word embedding vector with a positional encoding vector.

#+ATTR_ORG: :width 600
[[./img/position.png]]

** Residual connection around self attention layer

#+ATTR_ORG: :width 600
[[./img/norm.png]]

** Decoder

Topmost or the last encoder spits out the list of encoded vectors. Using which we create a set of key and query vectors. These are to be used in the encoder-decoder attention layer of every decoder module.

[[./img/decoder1.gif]]

The output from the top decoder at a time is fed into the bottom decoder in the next time step.
The self attention layer can only look at earlier positions so we mask the future positions by setting it to -inf before softmax step.

Encoder-decoder attention layer use query matrix generated from previous layer, but the key and value matrix from the output of encoder stack.

After the decoder stack there is a usual *decoder stack o/p->linear->logits->softmax* layers. The number with the highest prob indicate the corresponding word.
* Summary
Let number of words in the sentence be $s$.
Let length of embedding vector be $e$.

** Encoder

*** Positional encoding
Each vector should have info regarding it's position within the sentence.

\begin{equation*}
X = X + P
\end{equation*}

*** Self Attention
Then input word embedding matrix would be $\underset{s\times e}{X}$.
Let length of key, query and value vectors as $h$.

\begin{equation*}
\underset{s\times h}{Q_i} = {\underset{s\times e}{X}}\times \underset{e\times h}{W_i^Q}
\end{equation*}

\begin{equation*}
\underset{s\times h}{K_i} = {\underset{s\times e}{X}}\times \underset{e\times h}{W_i^K}
\end{equation*}

\begin{equation*}
\underset{s\times h}{V_i} = {\underset{s\times e}{X}}\times \underset{e\times h}{W_i^V}
\end{equation*}

Where $i = \{1,2,..H\}$.
$H$ is the number of heads present in the multihead attention layer.

\begin{equation*}
\underset{s\times s}{S_i} = softmax(\frac{\underset{s\times h}{Q_i}\times \underset{h\times s}{K_i^T}}{\sqrt{h}})
\end{equation*}

\begin{equation*}
\underset{s\times h}{Z_i} = \underset{s\times s}{S_i}\times \underset{s\times h}{V_i}
\end{equation*}

\begin{equation*}
\underset{s\times H}{Z} = Z_1 || Z_2 ||...||Z_H
\end{equation*}

\begin{equation*}
\underset{s\times h}{Z} = \underset{s\times H}{Z}\times \underset{H\times h}{W^Z}
\end{equation*}

\begin{equation*}
\underset{s\times h}{Z} = LayerNorm(X+Z)
\end{equation*}


In summary:

\begin{equation*}

\begin{equation*}
\underset{s\times h}{Z} = selfAtten(\underset{s\times h}{Q}, \underset{s\times h}{K}, \underset{s\times h}{V})
\end{equation*}

*** Feed forward neural network
\begin{equation*}
\underset{1\times h}{r} = NN{(\underset{1\times h}{z})
\end{equation*}

$\forall r_i , i=\{1,2,..s\}$ parallely with same $NN$.

\begin{equation*}
z = LayerNorm(r+z)
\end{equation*}

\begin{equation*}
\underset{s\times h}{K_d} = \underset{s\times h}{R}\times \underset{h\times h}{W_d^K}$
\end{equation*}

\begin{equation*}
\underset{s\times h}{V_d} = \underset{s\times h}{R}\times \underset{h\times h}{W_d^V}
\end{equation*}

** Decoder
*** Self Attention

\begin{equation*}
O = O + P
\end{equation*}


$\underset{s\times e}{O} = \underset{s\times e}{M} + \underset{s\times e}{O}$

where M is a look ahead mask matrix with upper triangular elements as $-inf$.

\begin{equation*}
\underset{s\times h}{Q_i} = {\underset{s\times e}{O}}\times \underset{e\times h}{W_i^Q}
\end{equation*}

\begin{equation*}
\underset{s\times h}{K_i} = {\underset{s\times e}{O}}\times \underset{e\times h}{W_i^K}
\end{equation*}

\begin{equation*}
\underset{s\times h}{V_i} = {\underset{s\times e}{O}}\times \underset{e\times h}{W_i^V}
\end{equation*}


\begin{equation*}
\underset{s\times h}{Z} = selfAtten(\underset{s\times h}{Q}, \underset{s\times h}{K}, \underset{s\times h}{V})
\end{equation*}

*** Encoder-Decoder Attention Layer

\begin{equation*}
\underset{s\times h}{Q_i} = {\underset{s\times h}{Z}}\times \underset{h\times h}{W_i^Q}
\end{equation*}

\begin{equation*}
\underset{s\times h}{Z} = selfAtten(\underset{s\times h}{Q}, \underset{s\times h}{K_d}, \underset{s\times h}{V_d})
\end{equation*}

*** Feed forward neural network
\begin{equation*}
\underset{1\times h}{r} = NN{(\underset{1\times h}{z})
\end{equation*}

$\forall r_i , i=\{1,2,..s\}$ parallely with same $NN$.

\begin{equation*}
z = LayerNorm(r+z)
\end{equation*}

** Training
We prepend and append special tokens to the input sentence of encoder, ie <SOS> and <EOS>.
Input of decoder is the target output with <SOS> preppended. If the number of words are smaller we pad it to equalize the sequence length of the model.
Output target sentence will be appended by <EOS>.


