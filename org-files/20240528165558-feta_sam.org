:PROPERTIES:
:ID:       566fe5b0-c027-498d-b82b-67ce5e583ae3
:END:
#+title: Feta-SAM

Experimental model architecture for real-time segmentation of Fetal ultrasound images.

* SAM+Agg
** Necessary Imports
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam2.py :makedirp yes
import torch
import torch.nn as nn
import numpy as np
from segment_anything import sam_model_registry
#+end_src

** Layer norm
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam2.py :makedirp yes
class LayerNorm2d(nn.Module):
    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:
        super().__init__()
        self.weight = nn.Parameter(torch.ones(num_channels))
        self.bias = nn.Parameter(torch.zeros(num_channels))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        u = x.mean(1, keepdim=True)
        s = (x - u).pow(2).mean(1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.eps)
        x = self.weight[:, None, None] * x + self.bias[:, None, None]
        return x
#+end_src

** Aggregator
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam2.py :makedirp yes
class AggregatorBlock(nn.Module):
    def __init__(self,m,n):
        super(AggregatorBlock, self).__init__()
        self.downconv = nn.Sequential(
            nn.Conv2d(m, n, kernel_size=(1, 1), stride=(1, 1), bias=False),
            LayerNorm2d(n),
            nn.GELU(approximate='none'))
        self.conv = nn.Sequential(
            nn.Conv2d(n, n, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),
            LayerNorm2d(n),
            nn.GELU(approximate='none'))

    def forward(self, x, m_prev=None, phi_prev=None):
        x = x.permute(0,3,1,2)

        f = self.downconv(x)
        if m_prev is None and phi_prev is None:
            m = f
        else:
            m = f + phi_prev + m_prev
        phi = self.conv(m)
        return phi,m

class Aggregator(nn.Module):
    def __init__(self):
        super(Aggregator, self).__init__()
        self.aggregators = nn.ModuleList()
        for i in range(0,6):
            agg = AggregatorBlock(768,512)
            self.aggregators.append(agg)

        self.fusion = nn.Sequential(
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1,1), bias=False),
            LayerNorm2d(512),
            nn.GELU(approximate='none'),
            nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),
            LayerNorm2d(256),
            nn.GELU(approximate='none'),
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),
            LayerNorm2d(256),
            nn.GELU(approximate='none'),
            nn.Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),
            LayerNorm2d(256),
            nn.GELU(approximate='none'),)

    def forward(self, x):
        phi,m = self.aggregators[0](x["0"])
        for i in range(1,6):
            phi,m = self.aggregators[i](x[str(i)],m, phi)
        out = self.fusion(phi)
        return out
#+end_src

** FetaSAM
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam2.py :makedirp yes
class FetaSam(nn.Module):
    def __init__(self, model_type, sam_checkpoint):
        super(FetaSam, self).__init__()
        self.activation = {}        # select sam model type and load weights
        self.model_type = model_type 
        self.sam_checkpoint = sam_checkpoint
        self.model = sam_model_registry[model_type](checkpoint=sam_checkpoint)
        self.model.image_encoder.eval()
        print("Sam weights loaded")
        self.hooks = []
        for i in range(0,6):
            self.hooks.append(self.model.image_encoder.blocks[i].register_forward_hook(self.getActivation(str(i))))
        self.aggregator = Aggregator().cuda()

        with torch.no_grad():
            self.sparse_embeddings_none, self.dense_embeddings_none = self.model.prompt_encoder(points=None, boxes=None, masks=None)
 
    def getActivation(self,name):
        # the hook signature
        def hook(model, input, output):
            self.activation[name] = output.detach()
        return hook
        
    @torch.no_grad()
    def encode_img(self, x):
        # (B,3,1024,1024) -> (B,256,64,64)
        img_embed = self.model.image_encoder(x) 
        return img_embed

    def decode_mask(self, img_embed, prompt_embed):
        pred, iou = self.model.mask_decoder(
            image_embeddings=img_embed,
            image_pe=self.model.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=self.sparse_embeddings_none.cuda(),
            dense_prompt_embeddings=prompt_embed,
            multimask_output=False) #(1,256,256)
        return pred
 
    def forward(self, x):
        img_embed = self.encode_img(x)
        prompt_embed = self.aggregator(self.activation)
        mask = self.decode_mask(img_embed, prompt_embed)
        return mask
#+end_src

**  test
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam2.py :makedirp yes
if __name__ == "__main__":
    import torch
    from torchinfo import summary
    model = FetaSam(
        model_type="vit_b",
        sam_checkpoint="/media/lfz/New Volume/ultrasound/weights/sam_vit_b.pth").cuda()
    x = torch.randn(2, 3, 1024, 1024).cuda()
    o = model(x)
    # print(model)
    print(x.size(), o.size())
#+end_src

* MobileSAM+Agg
** Necessary Imports
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam.py :makedirp yes
import torch
import torch.nn as nn
import numpy as np
from mobile_sam import sam_model_registry
import math
#+end_src

** Layer norm
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam.py :makedirp yes
class LayerNorm2d(nn.Module):
    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:
        super().__init__()
        self.weight = nn.Parameter(torch.ones(num_channels))
        self.bias = nn.Parameter(torch.zeros(num_channels))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        u = x.mean(1, keepdim=True)
        s = (x - u).pow(2).mean(1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.eps)
        x = self.weight[:, None, None] * x + self.bias[:, None, None]
        return x
#+end_src

** Aggregator
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam.py :makedirp yes
class AggregatorBlock(nn.Module):
    def __init__(self,m,n):
        super(AggregatorBlock, self).__init__()
        self.downconv = nn.Sequential(
            nn.Conv2d(m, n, kernel_size=(1, 1), stride=(1, 1), bias=False),
            LayerNorm2d(n),
            nn.GELU(approximate='none'))
        self.conv = nn.Sequential(
            nn.Conv2d(n, n, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),
            LayerNorm2d(n),
            nn.GELU(approximate='none'))

    def forward(self, x, m_prev=None, phi_prev=None):
        B,k,C = x.size()
        kk = int(math.sqrt(k))
        x = x.view(B,kk,kk,C)
        x = x.permute(0,3,1,2)

        f = self.downconv(x)
        if m_prev is None and phi_prev is None:
            m = f
        else:
            m = f + phi_prev + m_prev
        phi = self.conv(m)
        return phi,m

class Aggregator(nn.Module):
    def __init__(self):
        super(Aggregator, self).__init__()
        self.conv0 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),
            LayerNorm2d(256),
            nn.GELU(approximate='none'),
            )
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

        self.conv1 = nn.Sequential(
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),
            LayerNorm2d(256),
            nn.GELU(approximate='none'),
            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),
            )

        self.aggregator1 = AggregatorBlock(160,256).cuda()
        self.aggregator2 = AggregatorBlock(320,256).cuda()
        self.aggregator3 = AggregatorBlock(320,256).cuda()
        self.fusion = nn.Sequential(
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1,1), bias=False),
            LayerNorm2d(256),
            nn.GELU(approximate='none'),
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),
            LayerNorm2d(256),
            nn.GELU(approximate='none'),
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),
            LayerNorm2d(256),
            nn.GELU(approximate='none'),
            nn.Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),
            LayerNorm2d(256),
            nn.GELU(approximate='none'),)

    def forward(self, xconv, x):
        B,k,C = x["0"].size()
        kk = int(math.sqrt(k))
        xx = x["0"].view(B,kk,kk,C)
        xx = xx.permute(0,3,1,2)
        m = self.conv0(xx)
        phi0 = self.conv1(m)
        m = self.maxpool(m)
        phi,m = self.aggregator1(x["1"],m,phi0)
        phi,m = self.aggregator2(x["2"],m, phi)
        phi,m = self.aggregator3(x["3"],m, phi)
        phi = phi + xconv + phi0
        out = self.fusion(phi)
        return out
#+end_src

** FetaSAM
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam.py :makedirp yes
class FetaSam(nn.Module):
    def __init__(self, model_type, sam_checkpoint):
        super(FetaSam, self).__init__()
        self.activation = {}        # select sam model type and load weights
        self.model_type = model_type 
        self.sam_checkpoint = sam_checkpoint
        self.model = sam_model_registry[model_type](checkpoint=sam_checkpoint)
        self.model.image_encoder.eval()
        print("MobileSam weights loaded")
        self.hooks = []
        self.hooks.append(self.model.image_encoder.patch_embed.register_forward_hook(self.getActivation("patch")))
        for i in range(0,4):
            self.hooks.append(self.model.image_encoder.layers[i].register_forward_hook(self.getActivation(str(i))))
        self.aggregator = Aggregator().cuda()
        with torch.no_grad():
            self.sparse_embeddings_none, self.dense_embeddings_none = self.model.prompt_encoder(points=None, boxes=None, masks=None)
        self.x_conv = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1,1), bias=False),
            LayerNorm2d(64),
            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1,1), bias=False),
            LayerNorm2d(128),
            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),
            nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1,1), bias=False),
            LayerNorm2d(256),
            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1,1), bias=False),
            LayerNorm2d(256),
            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),
            nn.ReLU())
 
    def getActivation(self,name):
        # the hook signature
        def hook(model, input, output):
            self.activation[name] = output.detach()
        return hook

        
    @torch.no_grad()
    def encode_img(self, x):
        # (B,3,1024,1024) -> (B,256,64,64)
        img_embed = self.model.image_encoder(x) 
        return img_embed

    def decode_mask(self, img_embed, prompt_embed):
        pred, iou = self.model.mask_decoder(
            image_embeddings=img_embed,
            image_pe=self.model.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=self.sparse_embeddings_none.cuda(),
            dense_prompt_embeddings=prompt_embed,
            multimask_output=False) #(1,256,256)
        return pred
 
    def forward(self, x):
        img_embed = self.encode_img(x)
        xconv = self.x_conv(x)
        prompt_embed = self.aggregator(xconv,self.activation)
        mask = self.decode_mask(img_embed, prompt_embed)
        return mask
#+end_src

**  test
#+begin_src python :tangle ~/projects/ultrasound/models/fetasam.py :makedirp yes
if __name__ == "__main__":
    import torch
    from torchinfo import summary
    model = FetaSam(
        model_type="vit_t",
        sam_checkpoint="/media/lfz/New Volume/ultrasound/weights/mobilesam_vit_b.pt").cuda()
    x = torch.randn(2, 3, 1024, 1024).cuda()
    print(model)
    o = model(x)
    print(x.size(), o.size())
#+end_src

* Mediscan 
** Why different resizing?
The `resize_longest_distance` function in the SAM (Segment Anything Model) code likely serves to standardize the input image size by resizing the longest dimension to a specific value while maintaining the aspect ratio. This approach is commonly used in computer vision for several reasons:

1. **Aspect Ratio Preservation**: By resizing only the longest dimension and scaling the other dimension proportionally, the function preserves the original aspect ratio of the image. This prevents distortion that could occur if the width and height were resized independently to fixed values. Maintaining the aspect ratio is crucial for the model to correctly interpret the features and objects in the image.

2. **Standardization**: Standardizing the longest dimension of images helps in creating a uniform input size for the model. This is important for batching multiple images together for efficient processing during training and inference. It simplifies the handling of images of different sizes.

3. **Efficiency**: Resizing based on the longest dimension can ensure that the resulting image size is manageable in terms of computational resources. Fixed resizing to both width and height could lead to very large or very small images, either of which can be problematic. Large images can consume excessive memory and computation power, while very small images might lose important details.

4. *Generalization*: Models trained on images resized with aspect ratio preservation tend to generalize better to real-world scenarios. Objects in natural images appear in various sizes and shapes, and preserving the aspect ratio helps the model learn more generalized features.

Here is a brief example to illustrate:

- *Original Image Size*: 4000 x 3000 (width x height)
- *Target Longest Dimension*: 1024

Using resize_longest_distance:
- The longest dimension (4000) is resized to 1024.
- The shorter dimension is scaled proportionally: $\( 3000 \times \frac{1024}{4000} = 768 \)$.

Thus, the new size becomes 1024 x 768, preserving the aspect ratio.

If we were to resize both dimensions to fixed values, say 1024 x 1024, it would distort the image:

- Original aspect ratio: $\( \frac{4000}{3000} = 1.33 \)$
- New aspect ratio: $\( \frac{1024}{1024} = 1.0 \)$

This distortion can negatively impact the model's performance.

In summary, the resize_longest_distance function is used to ensure that images are resized efficiently while maintaining their original aspect ratio, which is crucial for the model's performance and generalization.
** import
#+begin_src python :tangle ~/projects/ultrasound/dataloaders/mediscan.py :mkdirp yes
import os
import random
import pickle
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset
from torch.nn import functional as F
#+end_src

** setup data
#+begin_src python :tangle ~/projects/ultrasound/setup.py :mkdirp yes
import os
import random
import pickle
import sys

random.seed(1)

img_dir = "/media/lfz/New Volume/ultrasound/data/mediscan-seg"

# fpus23
if img_dir.split("/")[-1] == "fpus23-seg":
    imgs = {}
    parts = []
    for i in os.listdir(os.path.join(img_dir,"images")):
        img_path = os.path.join(img_dir, "images",i)
        part = i.split("_")[2]
        parts.append(part)
    for key in set(parts):
        imgs[key] = []

    for i in os.listdir(os.path.join(img_dir,"images")):
        img_path = os.path.join(img_dir, "images",i)
        part = i.split("_")[2]
        imgs[part].append(img_path)

    train = []
    test = []
    for label in imgs:
        random.shuffle(imgs[label])
        print(label, len(imgs[label]))
        train_split = imgs[label][:int(0.9*(len(imgs[label])))]
        train += train_split
        tmp = imgs[label][int(0.9*(len(imgs[label]))):]
        random.shuffle(tmp)
        test += tmp

# mediscan
if img_dir.split("/")[-1] == "mediscan-seg":
    imgs = {}
    for label in os.listdir(img_dir):
        imgs[label] = [os.path.join(img_dir, label, "images", x) for x in os.listdir(os.path.join(img_dir, label, "images"))]

    train = []
    test = []

    for label in imgs:
        random.shuffle(imgs[label])
        train_split = imgs[label][:int(0.9*(len(imgs[label])))]
        train += train_split
        tmp = imgs[label][int(0.9*(len(imgs[label]))):]
        random.shuffle(tmp)
        test += tmp

random.shuffle(train)
random.shuffle(test)
with open(os.path.join(img_dir,'train.pkl'), 'wb') as f:
    pickle.dump(train, f)
with open(os.path.join(img_dir,'test.pkl'), 'wb') as f:
    pickle.dump(test, f)

print(f'Train: {len(train)}; Test: {len(test)}')
#+end_src

** class
#+begin_src python :tangle ~/projects/ultrasound/dataloaders/mediscan.py :mkdirp yes
class Mediscan(Dataset):
    def __init__(self, img_paths, transform=None, sam_trans=None, target_length=1024):
        self.img_paths = img_paths
        self.transform = transform
        self.sam_trans = sam_trans
        self.target_length = target_length
        pixel_mean = [123.675, 116.28, 103.53]
        pixel_std = [58.395, 57.12, 57.375],
        self.pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)
        self.pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)


    def __len__(self):
        return len(self.img_paths)

    def apply_transform(self, image1, image2):
            return self.transform(image1), self.transform(image2)

    def pad_(self, x):
        h, w = x.shape[-2:]
        padh = self.target_length - h
        padw = self.target_length - w
        x = F.pad(x, (0, padw, 0, padh))
        return x
    
    def normalize_pad(self, x):
        """Normalize pixel values and pad to a square input."""
        # Normalize colors
        if x.shape[0]==1:
            pass
        else:
            x = (x - self.pixel_mean) / self.pixel_std
            # x = x / 255
            pass
        # Pad
        x = self.pad_(x)
        return x

    def __getitem__(self, idx):
        out = {}
        img_path = self.img_paths[idx]
        mask_path = img_path.replace("images", "masks")
        mask_path = mask_path.replace("png", "bmp")

        img_id = img_path.split("/")[-3]+"_"+img_path.split("/")[-1][:-4]

        img = cv2.imread(img_path)
        mask = cv2.imread(mask_path,0)

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #(H,W,3)
        img, mask = self.sam_trans.apply_image(img), self.sam_trans.apply_image(mask)
        if self.transform:
           img, mask = self.apply_transform(img, mask)

        mask[mask > 0.5] = 1
        mask[mask <= 0.5] = 0

        out["orig_size"] = torch.Tensor([img.size(1), img.size(2)])

        out["img_orig"] = self.pad_(img)
        out["mask_orig"] = self.pad_(mask)

        img, mask = self.normalize_pad(img), self.normalize_pad(mask)

        out["id"] = img_id
        out["img"] = img
        out["mask"] = mask
        return out
#+end_src

** test
#+begin_src python :tangle ~/projects/ultrasound/dataloaders/mediscan.py :mkdirp yes
if __name__ == "__main__":
    from torchvision import transforms as T
    from torchvision.utils import save_image
    # d = CustomDataset()
    proj_dir = "/home/lfz/projects/ultrasound"
    out_dir=os.path.join(proj_dir,"runs","fetasam_agg1")
    sam_input_size = 1024
    sam_trans = ResizeLongestSide(sam_input_size)
    transform = T.Compose([
        T.ToPILImage(),
        T.ColorJitter(brightness=0.4,
                    contrast=0.4,
                    saturation=0.4,
                    hue=0.1),
        T.RandomHorizontalFlip(),
        T.RandomAffine(22, scale=(0.75, 1.25)),
        T.ToTensor()])

    train, test = setup_data(img_dir=os.path.join(proj_dir,"data","mediscan-seg"), out_dir=out_dir)
    ds = Mediscan(train, transform, sam_trans)
    next(iter(ds))
#+end_src

* Train
** import
#+begin_src python :tangle ~/projects/ultrasound/train.py :mkdirp yes
import os
import sys
import wandb
from tqdm import tqdm
from dataloaders.mediscan import *
from models.fetasam2 import *
#from models.fetasam import *
from mobile_sam.utils.transforms import ResizeLongestSide
from torch.utils.data import DataLoader
from torchvision import transforms as T
import torch.optim as optim
from statistics import mean
import pickle
import torch.nn as nn
#+end_src

** Hyper parameters
#+begin_src python :tangle ~/projects/ultrasound/train.py :mkdirp yes
assert torch.cuda.is_available(), "GPU not available"
device = "cuda"

data_dir = "/media/lfz/New Volume/ultrasound"
target_length = 1024

sam_trans = ResizeLongestSide(target_length)
train_batch_size = 2
val_batch_size = 1

transform = T.Compose([
    T.ToPILImage(),
    T.ColorJitter(brightness=0.4,
                  contrast=0.4,
                  saturation=0.4,
                  hue=0.1),
    T.RandomHorizontalFlip(),
    T.RandomAffine(22, scale=(0.75, 1.25)),
    T.ToTensor()])
target_transform = T.Compose([
    T.ToPILImage(),
    T.ToTensor()])

model_type="vit_b"
sam_checkpoint=os.path.join(data_dir,"weights/sam_vit_b.pth")

config = {"lr": 1e-3,
          "weight_decay": 1e-4,
          "momentum": 0.9,
          "num_epochs": 100,
          "grad_clip": 1.0,
          "run_name": "sam_mediscan_agg_pretrained_sgd"}
#+end_src

** Dataloaders
#+begin_src python :tangle ~/projects/ultrasound/train.py :mkdirp yes
out_dir=os.path.join(data_dir,"runs",config["run_name"])
os.makedirs(out_dir, exist_ok=True)
os.makedirs(os.path.join(out_dir, "preds"), exist_ok=True)

# train, val = setup_data(img_dir=os.path.join(data_dir,"data","fpus23-seg"), out_dir=out_dir)
data_root = os.path.join(data_dir,"data", "mediscan-seg")
with open(os.path.join(data_root, 'train.pkl'), 'rb') as f:
    train = pickle.load(f)
with open(os.path.join(data_root, 'test.pkl'), 'rb') as f:
    val = pickle.load(f)
print(f'Train: {len(train)}; Test: {len(val)}')

train_dataset = Mediscan(train, transform, sam_trans)
train_dataloader = DataLoader(dataset=train_dataset, batch_size=train_batch_size, shuffle=True)
val_dataset = Mediscan(val, target_transform, sam_trans)
val_dataloader = DataLoader(dataset=val_dataset, batch_size=val_batch_size, shuffle=False)
#+end_src

** Load model
#+begin_src python :tangle ~/projects/ultrasound/train.py :mkdirp yes
model = FetaSam(model_type, sam_checkpoint).cuda()
ckpt = os.path.join(data_dir,"runs","sam_fpus_agg_sgd","fetasam_best.pth")
checkpoint = torch.load(ckpt)
model.load_state_dict(checkpoint["model"])
model.to(device)

# Freezing weights
for name, param in model.named_parameters():
    tmp = name.split(".")[1]
    if tmp in ["prompt_encoder","image_encoder", "mask_decoder"]:#,"conv0","conv1","fusion"] or "aggregator" in tmp:
        param.requires_grad_(False)

total_params  = sum(p.numel() for p in model.parameters())/1000000.0
train_params  = sum(p.numel() for p in model.parameters() if p.requires_grad)/1000000.0
print(f"Total Params: {total_params:.2f} M, Trainable Params: {train_params:.2f} M")
config["total_params"]=total_params
config["train_params"]=train_params
#sys.exit()
#+end_src

** Optimizer & Scheduler
#+begin_src python :tangle ~/projects/ultrasound/train.py :mkdirp yes
# optimizer = optim.Adam(model.parameters(), lr=config["lr"], weight_decay=config["weight_decay"])
optimizer = optim.SGD(model.parameters(), lr=config["lr"], momentum=config["momentum"])
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3)
#+end_src

** Wandb
#+begin_src python :tangle ~/projects/ultrasound/train.py :mkdirp yes
wandb.login()
wandb.init(
    project="sam",
    name = config["run_name"],
    config=config)
#+end_src

** Loss and metrics
#+begin_src python :tangle ~/projects/ultrasound/train.py :mkdirp yes
bce = nn.BCELoss()

def get_ji(predict, target):
    predict = predict + 1
    target = target + 1
    tp = np.sum(((predict == 2) * (target == 2)) * (target > 0))
    fp = np.sum(((predict == 2) * (target == 1)) * (target > 0))
    fn = np.sum(((predict == 1) * (target == 2)) * (target > 0))
    ji = float(np.nan_to_num(tp / (tp + fp + fn)))
    dice = float(np.nan_to_num(2 * tp / (2 * tp + fp + fn)))
    return ji,dice

def norm_batch(x):
    bs = x.shape[0]
    Isize = x.shape[-1]
    min_value = x.view(bs, -1).min(dim=1)[0].repeat(1, 1, 1, 1).permute(3, 2, 1, 0).repeat(1, 1, Isize, Isize)
    max_value = x.view(bs, -1).max(dim=1)[0].repeat(1, 1, 1, 1).permute(3, 2, 1, 0).repeat(1, 1, Isize, Isize)
    x = (x - min_value) / (max_value - min_value + 1e-6)
    return x

def get_dice_loss(y_pred, y_true, smooth=1):
    alpha = 0.5
    beta = 0.5

    tp = torch.sum(y_true * y_pred, dim=(1, 2, 3))
    fn = torch.sum(y_true * (1 - y_pred), dim=(1, 2, 3))
    fp = torch.sum((1 - y_true) * y_pred, dim=(1, 2, 3))
    tversky_class = (tp + smooth) / (tp + alpha * fn + beta * fp + smooth)
    return 1 - torch.mean(tversky_class)

def get_loss(pred, mask):
    mask = F.interpolate(mask, (256,256), mode='nearest')
    dice_loss = get_dice_loss(pred, mask)
    bce_loss = bce(pred, mask)
    return bce_loss, dice_loss

#+end_src

** Save predictions
#+begin_src python :tangle ~/projects/ultrasound/train.py :mkdirp yes
def save_img(batch, pred):
    pred = F.interpolate(pred, (1024, 1024), mode="bilinear", align_corners=False)
    pred = pred.detach().cpu()
    pred[pred<=0.5]=0.0
    pred[pred>0.5]=1.0

    N = batch["img"].size()[0]
    orig_sizes = batch["orig_size"].detach().cpu().numpy()
    
    for i in range(N):
        id_ = batch["id"][i]
        img = batch["img_orig"][i]
        mask = batch["mask_orig"][i]
        orig_size = orig_sizes[i].astype(int)
        img = torch.permute(img, (1, 2, 0)).detach().cpu().numpy()
        img = img[:orig_size[0], :orig_size[1],:]
        mask = torch.permute(mask, (1, 2, 0)).detach().cpu().numpy()
        mask = mask[:orig_size[0], :orig_size[1],:]

        p = pred[i]
        p = torch.permute(p, (1, 2, 0)).detach().cpu().numpy()
        p = p[:mask.shape[0], :mask.shape[1],:]

        overlay1 = np.concatenate((np.zeros((mask.shape[0], mask.shape[1], 2)), mask), axis=2)
        overlay1 = (overlay1*255).astype(np.uint8)

        overlay2 = np.concatenate((p, np.zeros((p.shape[0], p.shape[1], 2))), axis=2)
        overlay2 = (overlay2*255).astype(np.uint8)

        img = (img*255).astype(np.uint8)
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

        img1 = cv2.addWeighted(img,0.8, overlay1,0.2,0)
        img2 = cv2.addWeighted(img,0.8, overlay2,0.2,0)
        out = np.hstack((img1, img2))
       
        cv2.imwrite(os.path.join(out_dir, "preds", str(id_)+'.png'), out)

#+end_src

** Training
#+begin_src python :tangle ~/projects/ultrasound/train.py :mkdirp yes
best_iou = 0
wandb.watch(model, log="gradients", log_graph=False)
for epoch in range(config["num_epochs"]):
    print(f'EPOCH: {epoch+1}')
    wandb.log({"Epoch": epoch+1})

    pbar = tqdm(train_dataloader)

    model.train()
    losses = []
    bce_losses = []
    dice_losses = []

    optimizer.zero_grad()
    for i,batch in enumerate(pbar):
      # forward pass
      img = batch["img"].cuda()
      mask_gt = batch["mask"].cuda()

      mask_pred = model(img)
      mask_pred = norm_batch(mask_pred)
      bce_loss, dice_loss = get_loss(mask_pred, mask_gt)
      loss = bce_loss + dice_loss
      # backward pass (compute gradients of parameters w.r.t. loss)
      loss.backward()
      if (i+1)%3 == 0:
        nn.utils.clip_grad_norm_(model.parameters(), config["grad_clip"])
        optimizer.step()
        optimizer.zero_grad()

      wandb.log({"train_loss_step": loss.item()})
      pbar.set_postfix({'loss': loss.item()})
      losses.append(loss.item())
    mean_loss = mean(losses)
    print(f'train_loss_epoch: {mean_loss}')
    wandb.log({"train_loss_epoch": mean(losses)})


    model.eval()
    losses = []
    bce_losses = []
    dice_scores = []
    iou = []
    pbar = tqdm(val_dataloader)
    for batch in pbar:
      # forward pass
      img = batch["img"].cuda()
      mask_gt = batch["mask"].cuda()
      with torch.no_grad():
         mask_pred = model(img)

      mask_pred = norm_batch(mask_pred)
      bce_loss, dice_loss = get_loss(mask_pred, mask_gt)
      loss = bce_loss + dice_loss

      save_img(batch,mask_pred)
      pbar.set_postfix({'loss': loss.item()})
      losses.append(loss.item())

      mask_pred[mask_pred > 0.5] = 1.0
      mask_pred[mask_pred <= 0.5] = 0.0

      mask_gt = F.interpolate(mask_gt, (256,256), mode='nearest')
      ji,dice = get_ji(mask_pred.squeeze(1).detach().cpu().numpy(), mask_gt.squeeze(1).detach().cpu().numpy())
      iou.append(ji)
      dice_scores.append(dice)

    mean_loss = mean(losses)
    iou_e = mean(iou)
    mean_dice = mean(dice_scores)

    scheduler.step(mean_loss)
    lr_curr = scheduler.get_last_lr()[0]
    print("current lr: ", lr_curr)

    print(f'val_loss_epoch: {mean_loss}')
    print(f'val_iou_epoch: {iou_e}')
    wandb.log({"val_loss_epoch": mean_loss})
    wandb.log({"val_iou_epoch": iou_e})
    wandb.log({"val_dice_epoch": mean_dice})
    wandb.log({"learning_rate": lr_curr})
    
    if iou_e > best_iou:
        print(f"New best IoU : {iou_e}")
        checkpoint = {
            'epoch': epoch+1,
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'lr': lr_curr,
            'dice': mean_dice,
            'iou': iou_e
        }
        torch.save(checkpoint, os.path.join(out_dir,"fetasam_best.pth"))
        best_iou = iou_e
    wandb.log({"val_best_iou": best_iou})
#+end_src

* Move fpus23 masks to standalone fpus23-seg 
#+begin_src python 
import os
import shutil

data_dir = "/home/lfz/projects/ultrasound/data/fpus23/Dataset/masks"
dest_dir = "/media/lfz/New Volume/ultrasound/data/fpus23-seg"

for root, dirs, files in os.walk(data_dir):
        for file in files:
                if file.endswith(".bmp"):
                        x = root.split("/")
                        mask_name = x[-2]+"_"+x[-1]+"_"+file
                        img_root = root.replace("masks", "four_poses")
                        img_name = x[-2]+"_"+x[-1]+"_"+file[:-4]+".png"
                        print(img_name, mask_name)

                        shutil.copy2(
                                os.path.join(root,file),
                                os.path.join(dest_dir,"masks", mask_name))

                        shutil.copy2(
                                img_root+".png",
                                os.path.join(dest_dir,"images", img_name))
#+end_src

* Create overlay for fpus23-seg
#+begin_src python 
import os
import cv2
import numpy as np

data_dir = "/home/lfz/projects/ultrasound/data/fpus23-seg"
dest_dir = "/media/lfz/New Volume/fpus23-seg"

for path in os.listdir(data_dir+"/images"):
    img_path = os.path.join(data_dir+"/images", path)
    mask_path = os.path.join(data_dir+"/masks", path.replace("png", "bmp"))

    img = cv2.imread(img_path)
   mask = cv2.imread(mask_path,0)
    mask[mask>0]=1
    mask = np.expand_dims(mask, axis=2)
    overlay = np.concatenate((mask, np.zeros((mask.shape[0], mask.shape[1], 2))), axis=2)
    overlay = (overlay*255).astype(np.uint8)

    out = cv2.addWeighted(img,0.8, overlay,0.2,0)

    cv2.imwrite(dest_dir+"/"+path, out)
    # break
#+end_src
