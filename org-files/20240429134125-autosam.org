:PROPERTIES:
:ID:       b9cdac99-0341-47a9-bf7a-59c1b6c87234
:END:
#+title: AutoSAM
#+STARTUP: latexpreview inlineimages

* Basics
- [[https://arxiv.org/pdf/2306.06370][paper]]

Original SAM is trained on natural images, hence it fails in case of out of distribution (OOD) images
like medical images. This paper introduces *Overloading* as the solution.
** Overloading
We train an auxillary network replacing the original prompt endcoder. During training we will
freeze the original SAM and only train the new auxillary encoder.

[[./img/autosam.png]]

* Setup
- [[https://github.com/talshaharabany/AutoSAM][Github]]
#+begin_src sh
git clone https://github.com/talshaharabany/AutoSAM.git
#+end_src

* Architecture

[[./img/autosam2.png]]


SAM has three components:
1. Robust image encoder $E_s$ for image $I$
2. Prompt encoder $E_M$ produce prompt embedding $Z$
3. Lightweight mask decoder $D_s$ produce $M_z$
then a SAM network $S$ can be summarized as

\begin{equation*}
M_z = S(I, Z)
\end{equation*}

One advantage of this modularity is the we can change Z into
different stuff. In autosam we replace $E_M$ with our own $g$ but the input is $I$ itself

\begin{equation*}
$Z_I = g(I)$
\end{equation*}

* Loss
\begin{equation*}
L_{seg} = L_{BCE}(I,Z_I,M) + L_{dice}(I,Z_I,M)
\end{equation*}

$g$ has a encoder arm based on [[https://arxiv.org/abs/1909.00948][Harmonic Dense Net]] and a small convolutional decoder.
- Encoder gives a sequence of images with channels of size 192, 256, 320, 480, 720 and 1280.
- Decoder gives an output of size 64x64x256, which would be our $Z_I$
  
We can also train a /surrogate decoder/ $h$, hence $h(g(I))$ can be a lightweight alternative to SAM.

* Module

#+begin_src python :tangle ~/projects/ultrasound/models/autosam.py :mkdirp yes
import os
import torch
import numpy as np
import cv2
import torch.nn as nn
import lightning as L
import torch.nn.functional as F
from segment_anything import sam_model_registry
from .selfpromptencoder import ModelEmb

class AutoSAMModel(nn.Module):
    def __init__(self, model_type ,sam_checkpoint):
        super(AutoSAMModel, self).__init__()
        self.model_type = model_type 
        self.sam_checkpoint = sam_checkpoint
        self.sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
        self.sam.cuda()
        self.sam.eval()
        print("Pretrained SAM loaded!")

        self.self_prompt_encoder = ModelEmb().cuda()
        print("Pretrained Self-prompt encoder loaded!")

    def forward(self, x): # (B,3,1024,1024)
        xx = F.interpolate(x, (256, 256), mode='bilinear', align_corners=True)
        prompt_embed = self.self_prompt_encoder(xx)

        with torch.no_grad():
            x = torch.stack([self.sam.preprocess(img) for img in x], dim=0)
            img_embed = self.sam.image_encoder(x) # (B, 256,64,64)
            sparse_embeddings_none, dense_embeddings_none = self.sam.prompt_encoder(points=None, boxes=None, masks=None)

        pred, iou = self.sam.mask_decoder(
            image_embeddings=img_embed,
            image_pe=self.sam.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings_none.cuda(),
            dense_prompt_embeddings=prompt_embed,
            multimask_output=False) #(1,256,256)
        return pred

class AutoSAM(L.LightningModule):
    def __init__(self,
                 model_type,
                 checkpoint,
                 lr=1e-5):
        super().__init__()

        self.model_type = model_type
        self.checkpoint = checkpoint
        self.lr = lr
        print(self.lr)
        self.save_hyperparameters()
        self.model = AutoSAMModel(model_type, checkpoint)
        self.bce_loss = nn.BCELoss()

        # Freezing weights
        for name, param in self.model.named_parameters():
            if name.split('.')[1] == "image_encoder" or name.split('.')[1] == "prompt_encoder" or name.split('.')[1] == "mask_decoder":
                param.requires_grad_(False)

    def training_step(self, batch, batch_idx):
        img = batch["img"] # (B, 3, 1024, 1024)
        mask = batch["mask"] # (B, 1, 1024, 1024)

        pred = self.model(img) # (B, 1, 256, 256)
        pred = self.norm_batch(pred)
        
        loss = self.get_loss(pred, mask)
        self.log("train_loss", loss, on_step=True, on_epoch=True, batch_size=pred.size(0))
        return loss

    def validation_step(self, batch, batch_idx):
        img = batch["img"]
        mask = batch["mask"]

        pred = self.model(img)
        pred = self.norm_batch(pred)

        Idim=256
        pred = F.interpolate(pred, (Idim, Idim), mode="bilinear", align_corners=False)
        mask = F.interpolate(mask, (Idim, Idim), mode='nearest')
        pred[pred > 0.5] = 1.0
        pred[pred <= 0.5] = 0.0
        dice, ji = self.get_dice_ji(pred.squeeze(1).detach().cpu().numpy(), mask.squeeze(1).detach().cpu().numpy())
        self.save_img(batch, pred, "out")
        B = pred.size(0)
        self.log("val_loss", 1-dice, on_step=True, on_epoch=True, batch_size=B)
        self.log("val_dice", dice, on_step=False, on_epoch=True, batch_size=B)
        self.log("val_iou", ji, on_step=False, on_epoch=True, batch_size=B)

    def norm_batch(self, x):
        bs = x.shape[0]
        Isize = x.shape[-1]
        min_value = x.view(bs, -1).min(dim=1)[0].repeat(1, 1, 1, 1).permute(3, 2, 1, 0).repeat(1, 1, Isize, Isize)
        max_value = x.view(bs, -1).max(dim=1)[0].repeat(1, 1, 1, 1).permute(3, 2, 1, 0).repeat(1, 1, Isize, Isize)
        x = (x - min_value) / (max_value - min_value + 1e-6)
        return x

    def get_dice_ji(self,predict, target):
        predict = predict + 1
        target = target + 1
        tp = np.sum(((predict == 2) * (target == 2)) * (target > 0))
        fp = np.sum(((predict == 2) * (target == 1)) * (target > 0))
        fn = np.sum(((predict == 1) * (target == 2)) * (target > 0))
        ji = float(np.nan_to_num(tp / (tp + fp + fn)))
        dice = float(np.nan_to_num(2 * tp / (2 * tp + fp + fn)))
        return dice, ji

    def dice_loss(self, y_pred, y_true, smooth=1):
        alpha = 0.5
        beta = 0.5

        tp = torch.sum(y_true * y_pred, dim=(1, 2, 3))
        fn = torch.sum(y_true * (1 - y_pred), dim=(1, 2, 3))
        fp = torch.sum((1 - y_true) * y_pred, dim=(1, 2, 3))
        tversky_class = (tp + smooth) / (tp + alpha * fn + beta * fp + smooth)
        return 1 - torch.mean(tversky_class)

    def get_loss(self, pred, mask):
        mask = F.interpolate(mask, (256,256), mode='nearest')
        loss = self.bce_loss(pred, mask) + self.dice_loss(pred, mask)
        return loss
        
    def save_img(self, batch, pred, dest):
        pred = F.interpolate(pred, (1024, 1024), mode="bilinear", align_corners=False)
        pred = pred.detach().cpu()
        pred[pred<=0.5]=0.0
        pred[pred>0.5]=1.0
         
        N = batch["img"].size()[0]
        for i in range(N):
            img = batch["img"][i]
            img = torch.permute(img, (1, 2, 0)).detach().cpu().numpy()
            mask = batch["mask"][i]
            mask = torch.permute(mask, (1, 2, 0)).detach().cpu().numpy()
            id = batch["id"][i]
            p = pred[i]
            p = torch.permute(p, (1, 2, 0)).detach().cpu().numpy()

            
            overlay1 = np.concatenate((np.zeros((mask.shape[0], mask.shape[1], 2)), mask), axis=2)
            overlay1 = (overlay1*255).astype(np.uint8)

            overlay2 = np.concatenate((p, np.zeros((p.shape[0], p.shape[1], 2))), axis=2)
            overlay2 = (overlay2*255).astype(np.uint8)

            img = (img*255).astype(np.uint8)
            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

            img1 = cv2.addWeighted(img,0.8, overlay1,0.2,0)
            img2 = cv2.addWeighted(img,0.8, overlay2,0.2,0)
            out = np.hstack((img1, img2))
            cv2.imwrite(os.path.join(dest, str(id)+'.png'), out)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.model.self_prompt_encoder.parameters(), lr=self.lr, weight_decay=1e-4)
        return optimizer

if __name__ == "__main__":
    import torch
    model = AutoSAMModel(model_type="vit_b", sam_checkpoint="/home/lfz/projects/model_weights/autosam_vit_b.pth").cuda()

    # # model = PromptEncoder().cuda()
    # # x = torch.randn(2,4096, 320).cuda()
    # # o = model(x)
    # # print(o.size())

    x = torch.randn(2, 3, 1024, 1024).cuda()
    o = model(x)
    print(o.size())

    model = AutoSAM(model_type="vit_b", checkpoint="/home/lfz/projects/model_weights/autosam_vit_b.pth").cuda()
#+end_src

* Config
#+begin_src yaml :tangle ~/projects/ultrasound/configs/autosam.yml :mkdirp yes
trainer:
  callbacks:
     class_path: lightning.pytorch.callbacks.ModelCheckpoint
     init_args:
        dirpath: 'checkpoints'
        filename: 'autosam-{epoch:02d}-{val_loss:.2f}'
        save_top_k: 2
        monitor: 'val_loss'
        mode: 'min'
        save_last: True
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      save_dir: 'wandb'
      project: 'sam'
      name: 'autosam-g-lrplus'
      dir: 'wandb'
  max_epochs: 100
model:
  model_type: vit_b
  checkpoint: /home/lfz/projects/model_weights/autosam_vit_b.pth
  lr: 0.0003
data:
  data_dir: /home/lfz/projects/data/mediscan-seg
  resize_dim: 256
  batch_size: 2
#+end_src

#+begin_src yaml :tangle ~/projects/ultrasound/configs/debug.yml :mkdirp yes
trainer:
  overfit_batches: 10
  max_epochs: 2
model:
  model_type: vit_b
  checkpoint: /home/lfz/projects/model_weights/autosam_vit_b.pth
  lr: 0.0003
data:
  data_dir: /home/lfz/projects/data/mediscan-seg
  resize_dim: 256
  batch_size: 2
#+end_src
