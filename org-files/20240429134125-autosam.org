:PROPERTIES:
:ID:       b9cdac99-0341-47a9-bf7a-59c1b6c87234
:END:
#+title: AutoSAM
#+STARTUP: latexpreview inlineimages


* Basics
- [[https://arxiv.org/pdf/2306.06370][paper]]

Original SAM is trained on natural images, hence it fails in case of out of distribution (OOD) images
like medical images. This paper introduces *Overloading* as the solution.
** Overloading
We train an auxillary network replacing the original prompt endcoder. During training we will
freeze the original SAM and only train the new auxillary encoder.

[[./img/autosam.png]]

* Setup
- [[https://github.com/talshaharabany/AutoSAM][Github]]
#+begin_src sh
git clone https://github.com/talshaharabany/AutoSAM.git
#+end_src

* Methodology

SAM has three components:
1. Robust image encoder $E_s$ for image $I$
2. Prompt encoder $E_M$ produce prompt embedding $Z$
3. Lightweight mask decoder $D_s$ produce $M_z$
then a SAM network $S$ can be summarized as

\begin{equation*}
M_z = S(I, Z)
\end{equation*}

One advantage of this modularity is the we can change Z into
different stuff. In autosam we replace $E_M$ with our own $g$ but the input is $I$ itself

\begin{equation*}
$Z_I = g(I)$
\end{equation*}

** Loss
\begin{equation*}
L_{seg} = L_{BCE}(I,Z_I,M) + L_{dice}(I,Z_I,M)
\end{equation*}

** Architecture
$g$ has a encoder arm based on [[https://arxiv.org/abs/1909.00948][Harmonic Dense Net]] and a small convolutional decoder.
- Encoder gives a sequence of images with channels of size 192, 256, 320, 480, 720 and 1280.
- Decoder gives an output of size 64x64x256, which would be our $Z_I$
  
We can also train a /surrogate decoder/ $h$, hence $h(g(I))$ can be a lightweight alternative to SAM.

* Check inference time
#+begin_src python :tangle ~/projects/autosam/test.py :mkdirp yes
import os
import kornia
import numpy as np
import cv2
import matplotlib.pyplot as plt
import torch
import argparse
import pickle
from models.model_single import ModelEmb
from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator
from segment_anything.utils.transforms import ResizeLongestSide
from torchvision.transforms import v2 as T
import random
from torchvision.utils import draw_segmentation_masks as draw_overlay
from torchvision.utils import save_image
import torch.nn.functional as F

os.environ.pop("QT_QPA_PLATFORM_PLUGIN_PATH")

parser = argparse.ArgumentParser(description='Description of your program')
parser.add_argument('-lr', '--learning_rate', default=0.0003, help='learning_rate', required=False)
parser.add_argument('-bs', '--Batch_size', default=2, help='batch_size', required=False)
parser.add_argument('-epoches', '--epoches', default=200, help='number of epoches', required=False)
parser.add_argument('-nW', '--nW', default=0, help='evaluation iteration', required=False)
parser.add_argument('-nW_eval', '--nW_eval', default=0, help='evaluation iteration', required=False)
parser.add_argument('-WD', '--WD', default=1e-4, help='evaluation iteration', required=False)
parser.add_argument('-task', '--task', default='monu', help='evaluation iteration', required=False)
parser.add_argument('-depth_wise', '--depth_wise', default=False, help='image size', required=False)
parser.add_argument('-order', '--order', default=85, help='image size', required=False)
parser.add_argument('-Idim', '--Idim', default=256, help='image size', required=False)
parser.add_argument('-rotate', '--rotate', default=22, help='image size', required=False)
parser.add_argument('-scale1', '--scale1', default=0.75, help='image size', required=False)
parser.add_argument('-scale2', '--scale2', default=1.25, help='image size', required=False)
args = vars(parser.parse_args())
sam_args = {
    'sam_checkpoint': "/home/lfz/projects/model_weights/sam_pretrained/sam_vit_b.pth",
    'model_type': "vit_b",
    'generator_args': {
        'points_per_side': 8,
        'pred_iou_thresh': 0.95,
        'stability_score_thresh': 0.7,
        'crop_n_layers': 0,
        'crop_n_points_downscale_factor': 2,
        'min_mask_region_area': 0,
        'point_grids': None,
        'box_nms_thresh': 0.7,
    },
    'gpu_id': 0,
}
def read_image(img_dir, img_path):
   file_path = os.path.join(img_dir, img_path)
   img_o = cv2.cvtColor(cv2.imread(file_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)
   img = transform_test(img_o)
   img = sam_trans.apply_image_torch(img)
   img = sam_trans.preprocess(img)
   img = img.unsqueeze(0)
   img = img.to(device=device)
   return img, img_o

def run_autosam(img, model, sam):
   dense_embeddings = model(img)
   image_embeddings = sam.image_encoder(img)
   pred, iou_predictions = sam.mask_decoder(
      image_embeddings=image_embeddings,
      image_pe=sam.prompt_encoder.get_dense_pe(),
      sparse_prompt_embeddings=sparse_embeddings_none,
      dense_prompt_embeddings=dense_embeddings,
      multimask_output=False)
   return pred

img_dir = "/home/lfz/projects/data/fetal_head_hc18/train/images"
test = os.listdir(img_dir)
# img_dir = "/home/lfz/projects/data/mediscan-seg"
# with open("test.pkl", "rb") as f:
#    test = pickle.load(f)
# with open("val.pkl", "rb") as f:
#    val = pickle.load(f)

# test = val + test

# load checkpoint
ckpt_path = "/home/lfz/projects/model_weights/autosam/net_best.pth"
model = torch.load(ckpt_path)
device = torch.device("cuda")
model.to(device)
model.eval()

# load SAM
sam = sam_model_registry[sam_args['model_type']](checkpoint=sam_args['sam_checkpoint'])
sam.to(device=device)
sam.eval()
sam_trans = ResizeLongestSide(sam.image_encoder.img_size)
sparse_embeddings_none, dense_embeddings_none = sam.prompt_encoder(points=None, boxes=None, masks=None)

Idim = 256
transform_test = T.Compose([
    T.ToPILImage(),
    T.Resize((Idim, Idim)),
    T.ToTensor(),
])

# GPU Warm-UP
print("GPU warming up...")
for img_path in test[:10]:
   img,_ = read_image(img_dir, img_path)
   with torch.no_grad():
      pred = run_autosam(img, model, sam)
print("Done!")

N = len(test)

starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
timings=np.zeros((N,1))
os.makedirs("overlay", exist_ok=True)
# MEASURE PERFORMANCE
with torch.no_grad():
  for idx in range(N):
     img,img_o = read_image(img_dir, test[idx])

     starter.record()
     pred = run_autosam(img, model, sam)
     ender.record()

     # WAIT FOR GPU SYNC
     torch.cuda.synchronize()
     curr_time = starter.elapsed_time(ender)
     timings[idx] = curr_time

mean_syn = np.mean(timings)
std_syn = np.std(timings)
print(mean_syn, std_syn)
#+end_src
