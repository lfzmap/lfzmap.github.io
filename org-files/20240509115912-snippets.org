:PROPERTIES:
:ID:       c64efc36-aefc-4aff-b7d8-9aedbbe10308
:END:
#+title: Snippets

** pytorch train 
#+begin_src emacs-lisp :tangle "~/.config/emacs/snippets/fundamental-mode/pytorch-train" :makedirp yes
# -*- mode: snippet -*-
# name: pytorch-train
# key: train
# --
import torch
import torch.nn as nn
import wandb
from dataset import get_dataset
from model import CustomModel
from torch.utils.data import DataLoader
from torch.optim import Adam

if not torch.cuda.is_available():
    raise Exception("CUDA not available!")
else:
    print("CUDA available")
    device = torch.device('cuda')

# Hyper parameters
lr = 1e-3
train_batch_size = 2
val_batch_size = 1
num_epochs = 100
img_dir = "/home/lfz/projects/ultrasound/datasets/mediscan-seg"
debug = False
if debug:
   num_epochs = 1

train_ds, val_ds, test_ds = get_dataset(img_dir, debug=debug)
train_dataloader = DataLoader(dataset=train_ds, batch_size=train_batch_size, shuffle=True)
val_dataloader = DataLoader(dataset=val_ds, batch_size=val_batch_size, shuffle=False)
test_dataloader = DataLoader(dataset=test_ds, batch_size=val_batch_size, shuffle=False)

model = CustomModel()
model.to(device)

optimizer = Adam(model.parameters(), lr=lr)
loss = nn.BCEWithLogitsLoss()

wandb.login()
wandb.init(
    project="testProject",
    config={
        "lr": 1e-5,
        "epochs": num_epochs,
    },)


os.makedirs("results", exist_ok=True)
os.makedirs("ckpts", exist_ok=True)
for epoch in range(num_epochs):
    print(f'EPOCH: {epoch}')
    epoch_losses = []
    model_g.train()
    for batch in tqdm(train_dataloader):
        pred = model_g(batch)

        optimizer.zero_grad()
        loss.backward()

        optimizer.step()
        epoch_losses.append(loss.item())
        wandb.log({"Train Loss batch": loss.item()})

    print(f'Train loss epoch: {mean(epoch_losses)}')
    wandb.log({"Train Loss epoch": mean(epoch_losses)})

    epoch_losses = []
    for batch in tqdm(val_dataloader):
        model.eval()
        with torch.no_grad():
            pred = model(batch)

        epoch_losses.append(loss.item())

    print(f'Val loss epoch: {mean(epoch_losses)}')
# Save the model's state dictionary to a file
torch.save(model_g.state_dict(), os.path.join("ckpts", "model.pth"))
#+end_src
** pytorch dataset
#+begin_src emacs-lisp :tangle "~/.config/emacs/snippets/fundamental-mode/pytorch-dataset" :makedirp yes
# -*- mode: snippet -*-
# name: pytorch dataset
# key: dataset
# --
import os
import torch
from torch.utils.data import Dataset
from torchvision.transforms import ToTensor

class $1CustomDataset(Dataset):
    def __init__(self, transform=None):
        self.transform = transform

    def __len__(self):
        return len()

    def __getitem__(self, idx):
        if self.transform:
           pass

        return 0

if __name__ == "__main__":
    d = CustomDataset()
#+end_src
** pytorch model
#+begin_src emacs-lisp :tangle "~/.config/emacs/snippets/fundamental-mode/pytorch-model" :makedirp yes
# -*- mode: snippet -*-
# name: pytorch model
# key: model
# --

import torch.nn as nn

class CustomModel(nn.Module):
    def __init__(self ):
        super(CustomModel, self).__init__()

    def forward(self, x):
        return 0
#+end_src

** Lightning module
LightningModule is the full recipe that defines how your nn.Modules interact.

#+begin_src emacs-lisp :tangle "~/.config/emacs/snippets/fundamental-mode/pl-module" :makedirp yes
# -*- mode: snippet -*-
# name: pl-module
# key: module
# --
import torch
import lightning as L
from model import CustomModel

class CustomModule(L.LightningModule):
    def __init__(self, model, lr):
        super().__init__()
        self.model = CustomModel()
        self.lr = lr
        self.save_hyperparameters(ignore=model)

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        x, y = batch
        z = self.model(x)
        loss = 
        return loss

    def validation_step(self, batch, batch_idx):
        # this is the validation loop
        x, y = batch
        z = self.model(x)
        val_loss = 
        self.log("val_loss", val_loss)

    def test_step(self, batch, batch_idx):
        # this is the test loop
        x, y = batch
        z = self.model(x)
        test_loss = 
        self.log("test_loss", test_loss)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer
#+end_src
** Dataset module
The LightningDataModule is a convenient way to manage data in PyTorch Lightning. It encapsulates training, validation, testing, and prediction dataloaders, as well as any necessary steps for data processing, downloads, and transformations.

#+begin_src emacs-lisp :tangle "~/.config/emacs/snippets/fundamental-mode/pl-datasetmodule" :makedirp yes
# -*- mode: snippet -*-
# name: pl-datasetmodule
# key: datasetmodule
# --

import lightning as L
from torch.utils.data import DataLoader
from torchvision import transforms as T


class CustomDataModule(L.LightningDataModule):
    def __init__(self, data_dir: str = "./"):
        super().__init__()
        self.data_dir = data_dir
        self.transform = T.Compose([T.ToTensor()])

    def prepare_data(self):
        # download
        pass

    def setup(self, stage: str):
        # Assign train/val datasets for use in dataloaders
        if stage == "fit":
           self.train_ds = 

        # Assign test dataset for use in dataloader(s)
        if stage == "test":

        if stage == "predict":

    def train_dataloader(self):
        return DataLoader(self.train_ds, batch_size=32)

    def val_dataloader(self):
        return DataLoader(self.val_ds, batch_size=32)

    def test_dataloader(self):
        return DataLoader(self.test_ds, batch_size=32)

    def predict_dataloader(self):
        return DataLoader(self.predict_ds, batch_size=32)
#+end_src

** Main calls for training

#+begin_src emacs-lisp :tangle "~/.config/emacs/snippets/fundamental-mode/pl-main" :makedirp yes
# -*- mode: snippet -*-
# name: pl-main
# key: main
# --

import lightning as L
from lightning.pytorch.cli import LightningCLI
from modules import *
from datamodules import *

cli = LightningCLI()

# model
model = CustomModule()

# train model
# saves checkpoints to 'some/path/' at every epoch end
trainer = L.Trainer(default_root_dir="some/path/")
datamodule = CustomDataModule()
trainer.fit(model, datamodule=datamodule)#fast_dev_run=True
# test the model
trainer.test(model, dataloaders=test_loader)
#+end_src
