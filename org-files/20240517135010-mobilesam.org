:PROPERTIES:
:ID:       652855c4-c6cb-476c-a8fd-39540e3e0c59
:END:
#+title: MobileSAM

* Basics

- [[https://arxiv.org/pdf/2306.14289][Paper]]
- [[https://github.com/ChaoningZhang/MobileSAM/tree/master][Code]]

#+begin_src sh
$pip install git+https://github.com/ChaoningZhang/MobileSAM.git
#+end_src

SAM's image encoder is replaced by a light-weight one.

#+ATTR_ORG: :width 800
[[./img/mobilesam.png]]

SAM's ViT based image encoder are huge, ViT-H = 632M and ViT-B = 86M. In this work a Tiny-ViT was created after knowledge distillation.

* Datamodule
#+begin_src python :tangle ~/projects/ultrasound/datamodules/mediscan.py :mkdirp yes
import os
import random
import pickle
import cv2
import lightning as L
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms as T
import torch.nn.functional as F

class MediscanDataset(Dataset):
  def __init__(self, img_paths, transform=None):
    self.img_paths = img_paths
    self.transform = transform

  def __len__(self):
    return len(self.img_paths)

  def __getitem__(self,idx):
    img_path = self.img_paths[idx]
    mask_path = img_path.replace("images", "masks")

    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    mask = cv2.imread(mask_path)
    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)

    if self.transform:
        img = self.transform(img)
        mask = self.transform(mask)

    img = F.interpolate(img.unsqueeze(0), 1024, mode="bilinear", align_corners=False, antialias=True).squeeze(0)
    mask = F.interpolate(mask.unsqueeze(0), 1024, mode="bilinear", align_corners=False, antialias=True).squeeze(0)

    mask[mask<0.5]=0.0
    mask[mask>=0.5]=1.0

    out = {"img": img, "mask":mask, "orig_shape": img.shape}
    out["id"] = img_path.split("/")[-3]+"_"+img_path.split("/")[-1][:-4]
    return out


class Mediscan(L.LightningDataModule):
    def __init__(self, data_dir, resize_dim, batch_size):
        super().__init__()
        self.data_dir = data_dir

        self.transform = T.Compose([
          T.ToPILImage(),
          T.Resize((resize_dim, resize_dim)),
          T.ColorJitter(brightness=0.4,
                        contrast=0.4,
                        saturation=0.4,
                        hue=0.1),
          T.RandomHorizontalFlip(),
          T.RandomAffine(22, scale=(0.75, 1.25)),
          T.ToTensor()])
        self.target_transform = T.Compose([
          T.ToPILImage(),
          T.Resize((resize_dim, resize_dim)),
          T.ToTensor()])

        self.batch_size=batch_size

    def prepare_data(self):
        imgs = {}
        img_dir = self.data_dir
        for label in os.listdir(img_dir):
            imgs[label] = [os.path.join(img_dir, label, "images", x) for x in os.listdir(os.path.join(img_dir, label, "images"))]

        train = []
        test = []

        for label in imgs:
            random.shuffle(imgs[label])
            train_split = imgs[label][:int(0.9*(len(imgs[label])))]
            train += train_split
            tmp = imgs[label][int(0.9*(len(imgs[label]))):]
            random.shuffle(tmp)
            test += tmp

        random.shuffle(train)
        random.shuffle(test)
        with open('train.pkl', 'wb') as f:
            pickle.dump(train, f)
        with open('test.pkl', 'wb') as f:
            pickle.dump(test, f)

        self.train = train
        self.test = test
        print(f'Train: {len(train)}; Test: {len(test)}')

    def setup(self, stage: str):
        # Assign train/val datasets for use in dataloaders
        if stage == "fit":
            self.train_ds = MediscanDataset(self.train, self.transform)
            self.val_ds = MediscanDataset(self.test, self.target_transform)
        if stage == "test":
            self.test_ds = MediscanDataset(self.test, self.target_transform)
        if stage == "predict":
            self.test_ds = MediscanDataset(self.test, self.target_transform)


    def train_dataloader(self):
        return DataLoader(self.train_ds, batch_size=self.batch_size)

    def val_dataloader(self):
        return DataLoader(self.val_ds, batch_size=self.batch_size)

    def test_dataloader(self):
        return DataLoader(self.test_ds, batch_size=self.batch_size)

    def predict_dataloader(self):
        return DataLoader(self.predict_ds, batch_size=self.batch_size)
#+end_src

* Module
#+begin_src python :tangle ~/projects/ultrasound/models/mobilesam.py :mkdirp yes
import os
import torch
import numpy as np
import cv2
import torch.nn as nn
import lightning as L
import torch.nn.functional as F
from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score
from mobile_sam.modeling.tiny_vit_sam import LayerNorm2d

class SelfPromptEncoder(nn.Module):
    def __init__(self):
        super(SelfPromptEncoder, self).__init__()
        self.neck = nn.Sequential(
            nn.Conv2d(
                320,
                256,
                kernel_size=1,
                bias=False,
            ),
            LayerNorm2d(256),
            nn.Conv2d(
                256,
                256,
                kernel_size=3,
                padding=1,
                bias=False,
            ),
            LayerNorm2d(256),)

    def forward(self, x): # (B,4096,320)
        B,_,C=x.size()
        x = x.view(B, 64, 64, C)
        x=x.permute(0, 3, 1, 2)
        y = self.neck(x)
        return y

class MobileSAMModel(nn.Module):
    def __init__(self, model_type ,sam_checkpoint):
        super(MobileSAMModel, self).__init__()
        self.model_type = model_type 
        self.sam_checkpoint = sam_checkpoint
        self.model = sam_model_registry[model_type](checkpoint=sam_checkpoint)
        with torch.no_grad():
            self.sparse_embeddings_none, self.dense_embeddings_none = self.model.prompt_encoder(points=None, boxes=None, masks=None)
        self.model.image_encoder.layers[3].register_forward_hook(self.hook)
        self.self_prompt_encoder = SelfPromptEncoder()
        self.model.eval()

    def hook(self, model, input, output):
        self.act1 = output.detach()

    def forward(self, x): # (B,3,1024,1024)
        with torch.no_grad():
            img_embed = self.model.image_encoder(x) # (B, 256,64,64)

        prompt_embed = self.self_prompt_encoder(self.act1)

        pred, iou = self.model.mask_decoder(
            image_embeddings=img_embed,
            image_pe=self.model.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=self.sparse_embeddings_none.cuda(),
            dense_prompt_embeddings=prompt_embed,
            multimask_output=False) #(1,256,256)
        return pred, iou

class MobileSAM(L.LightningModule):
    def __init__(self,
                 model_type,
                 checkpoint,
                 lr):
        super().__init__()

        self.model_type = model_type
        self.checkpoint = checkpoint
        self.lr = lr
        self.save_hyperparameters()
        self.mobilesam = MobileSAMModel(model_type, checkpoint)
        self.bce_loss = nn.BCELoss()
        # make sure we only compute gradients for mask decoder
        for name, param in self.mobilesam.named_parameters():
            if name.split('.')[1] == "image_encoder" or name.split('.')[1] == "prompt_encoder" or name.split('.')[1] == "mask_decoder":
                param.requires_grad_(False)
                print(name)

    def training_step(self, batch, batch_idx):
        img = batch["img"] # (B, 3, 1024, 1024)
        mask = batch["mask"] # (B, 1, 1024, 1024)
        pred, iou = self.mobilesam(img) # (B, 1, 256, 256)
        loss = self.get_loss(pred, mask)
        self.log("train_loss", loss, on_step=True, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        img = batch["img"]
        mask = batch["mask"]
        pred, iou = self.mobilesam(img)
        loss = self.get_loss(pred, mask)
        metrics = self.get_metrics(pred, mask)
        self.save_img(batch, pred, "out")
        self.log("val_loss", loss, on_step=False, on_epoch=True)
        self.log("val_iou", metrics[0], on_step=False, on_epoch=True)
        self.log("val_acc", metrics[1], on_step=False, on_epoch=True)
        self.log("val_recall", metrics[2], on_step=False, on_epoch=True)
        self.log("val_precision", metrics[3], on_step=False, on_epoch=True)
        self.log("val_f1", metrics[4], on_step=False, on_epoch=True)

    def test_step(self, batch, batch_idx):
        img = batch["img"]
        mask = batch["mask"]
        pred, iou = self.mobilesam(img)
        loss = self.get_loss(pred, mask)
        self.save_img(batch, pred, "out")
        self.log("test_iou", metrics[0], on_step=True, on_epoch=True)

    def dice_loss(self, y_pred, y_true, smooth=1):
        alpha = 0.5
        beta = 0.5

        tp = torch.sum(y_true * y_pred, dim=(1, 2, 3))
        fn = torch.sum(y_true * (1 - y_pred), dim=(1, 2, 3))
        fp = torch.sum((1 - y_true) * y_pred, dim=(1, 2, 3))
        tversky_class = (tp + smooth) / (tp + alpha * fn + beta * fp + smooth)
        return 1 - torch.mean(tversky_class)

    def get_metrics(self, pred, mask):
        pred = F.interpolate(pred, (1024, 1024), mode="bilinear", align_corners=False)
        pred = torch.sigmoid(pred).detach().cpu().numpy()
        mask = mask.detach().cpu().numpy()
        pred = (pred > 0.5).astype(np.uint8)
        intersection = np.logical_and(pred, mask)
        union = np.logical_or(pred, mask)
        iou = np.sum(intersection) / np.sum(union)
        y_true = mask.flatten()
        y_pred = pred.flatten()
        acc = accuracy_score(y_true, y_pred)
        recall= recall_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        return [iou,acc,recall,precision,f1]

    def get_loss(self, pred, mask):
        mask = F.interpolate(mask, (256, 256), mode="bilinear", align_corners=False)
        pred = torch.sigmoid(pred)

        loss = self.bce_loss(pred, mask) + self.dice_loss(pred, mask)
        return loss
        
    def save_img(self, batch, pred, dest):
        pred = F.interpolate(pred, (1024, 1024), mode="bilinear", align_corners=False)
        pred = torch.sigmoid(pred).detach().cpu()
        pred[pred<=0.5]=0.0
        pred[pred>0.5]=1.0
         
        N = batch["img"].size()[0]
        for i in range(N):
            img = batch["img"][i]
            img = torch.permute(img, (1, 2, 0)).detach().cpu().numpy()
            mask = batch["mask"][i]
            mask = torch.permute(mask, (1, 2, 0)).detach().cpu().numpy()
            id = batch["id"][i]
            p = pred[i]
            p = torch.permute(p, (1, 2, 0)).detach().cpu().numpy()

            
            overlay1 = np.concatenate((np.zeros((mask.shape[0], mask.shape[1], 2)), mask), axis=2)
            overlay1 = (overlay1*255).astype(np.uint8)

            overlay2 = np.concatenate((p, np.zeros((p.shape[0], p.shape[1], 2))), axis=2)
            overlay2 = (overlay2*255).astype(np.uint8)

            img = (img*255).astype(np.uint8)
            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

            img1 = cv2.addWeighted(img,0.8, overlay1,0.2,0)
            img2 = cv2.addWeighted(img,0.8, overlay2,0.2,0)
            out = np.hstack((img1, img2))
            cv2.imwrite(os.path.join(dest, str(id)+'.png'), out)



    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer

if __name__ == "__main__":
    import torch
    model = MobileSAMModel(model_type="vit_t",
                 sam_checkpoint="/home/lfz/projects/model_weights/mobile_sam.pt").cuda()
    # model = PromptEncoder().cuda()
    # x = torch.randn(2,4096, 320).cuda()
    # o = model(x)
    # print(o.size())

    x = torch.randn(2, 3, 1024, 1024).cuda()
    p, i = model(x)
    print(p.size())
#+end_src

#+RESULTS:

* Config
#+begin_src yaml :tangle ~/projects/ultrasound/configs/mobilesam.yml :mkdirp yes
trainer:
  callbacks:
     class_path: lightning.pytorch.callbacks.ModelCheckpoint
     init_args:
        dirpath: 'checkpoints'
        filename: 'mobilesam-{epoch:02d}-{val_loss:.2f}'
        save_top_k: 2
        monitor: 'val_loss'
        mode: 'min'
        save_last: True
     class_path: lightning.pytorch.callbacks.EarlyStopping
     init_args:
        monitor: 'val_loss'
        min_delta: 0.001
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      save_dir: 'wandb'
      project: 'sam'
      name: 'mobilesam'
      dir: 'wandb'
  max_epochs: 100
model:
  model_type: vit_t
  checkpoint: /home/lfz/projects/model_weights/mobile_sam.pt
  lr: 1e-5
data:
  data_dir: /home/lfz/projects/data/mediscan-seg
  resize_dim: 1024
  batch_size: 2
#+end_src
