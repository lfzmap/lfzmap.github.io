:PROPERTIES:
:ID:       c2ef2104-39a5-4e0b-a07a-425de90b641f
:END:
#+title: SAM

* Paper
- https://arxiv.org/pdf/2304.02643.pdf

* Basics

** Foundation Model
- [[https://arxiv.org/pdf/2302.09419.pdf][Survey paper]]
Also known as *Pretrained Foundation Models* (PFM). These are pretrained general models that can be finetuned to different downstream applications.
Eg: ChatGPT is fine tuned on pretrained GPT. Recently *Unified PFM* are introduced capable of handling multi-modal data.

** Zero-Shot Learning
-[[https://arxiv.org/pdf/2011.08641.pdf][Survey paper]] 

*Zero-shot learning (ZSL)* is a model's ability to detect classes never seen during training. The condition is that the classes are not known during supervised learning. 

* Installation
#+begin_src sh
$pip install git+https://github.com/facebookresearch/segment-anything.git
$pip install opencv-python pycocotools matplotlib onnxruntime onnx
#+end_src
* Download weights
- Download model checkpoints from [[https://github.com/facebookresearch/segment-anything?tab=readme-ov-file#model-checkpoints][here]].
  - h=Huge
  - l=Largw
  - B=base
* Usage
** Necessary imports 
#+begin_src python :tangle ~/projects/ultrasound/segmentation/sam/sam.py
import os
import pickle
import time
import cv2
import torch
import matplotlib.pyplot as plt
from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
import numpy as np

#+end_src
** Fix opencv-pyqt5 error
#+begin_src python :tangle ~/projects/ultrasound/segmentation/sam/sam.py
os.environ.pop("QT_QPA_PLATFORM_PLUGIN_PATH")

#+end_src
** Create overlay function
#+begin_src python :tangle ~/projects/ultrasound/segmentation/sam/sam.py
if not torch.cuda.is_available():
    raise Exception("CUDA not available!")
else:
    print("CUDA available")
    device = torch.device('cuda')

def show_anns(anns):
    if len(anns) == 0:
        return
    # sort as per area
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)
    # create zero matrix with alpha channel=1
    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:,:,3] = 0
    for ann in sorted_anns:
        m = ann['segmentation'] # get individual binary mask
        # create random color with alpha=0.35
        color_mask = np.concatenate([np.random.random(3), [0.35]])
        # replace True with color
        img[m] = color_mask
    ax.imshow(img)
    return img

#+end_src
** Setup SAM model
#+begin_src python :tangle ~/projects/ultrasound/segmentation/sam/sam.py
# sam model setup
sam = sam_model_registry["vit_l"](checkpoint="ckpts/sam_vit_l_0b3195.pth")
sam = sam.to(device=device)
mask_generator = SamAutomaticMaskGenerator(sam)

root = "/home/lfz/projects/ultrasound/datasets/mediscan-seg"
os.makedirs("./results", exist_ok=True)

#+end_src
** Generate and save SAM results
#+begin_src python :tangle ~/projects/ultrasound/segmentation/sam/sam.py
data = {}
for label in os.listdir(root):
    sub_root = os.path.join(root,label,"images")
    label_data = {}
    x_dicts = []
    label_data["num"] = len(os.listdir(sub_root))
    imgs = os.listdir(sub_root)
    for x in imgs: #os.listdir(sub_root):
        x_dict = {}
        x_dict["name"] = x
        img_path = os.path.join(sub_root,x)
        # read input image
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # plt.figure()
        plt.figure(figsize=(20,20))
        plt.imshow(img)
        plt.axis('off')
        # mask generation using sam
        start_ts = time.time()
        masks = mask_generator.generate(img)
        end_ts = time.time()
        t = (end_ts-start_ts)
        true_mask = cv2.imread(img_path.replace("images","masks"))
        true_mask = cv2.cvtColor(true_mask, cv2.COLOR_BGR2GRAY)
        true_mask[true_mask>=200]=255.0
        true_mask[true_mask<200]=0.0
        true_mask = true_mask.astype(bool)
        iou = []
        for ann in masks:
            m = ann['segmentation']
            intersection = np.logical_and(m, true_mask)
            union = np.logical_or(m, true_mask)
            iou_score = np.sum(intersection) / np.sum(union)
            iou.append(iou_score)
        iou = np.array(iou)
        iou_best = np.max(iou)
        idx = np.argmax(iou)
        best_mask = masks[idx]['segmentation']
        
        masks_overlay = show_anns(masks)
        plt.savefig(os.path.join("results", x), bbox_inches='tight')
        plt.close()
        print(f'{label}/{x} : iou = {iou_best:.3f} time = {t:.3f}')
        x_dict["infer_time"] = t
        x_dict["iou"] = iou_best
        x_dict["mask"] = best_mask
        x_dicts.append(x_dict)
    label_data["data"] = x_dicts
    data[label] = label_data

with open('sam_data.pkl', 'wb') as handle:
    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)
#+end_src
** Plot IoU histogram
#+begin_src python :tangle ~/projects/ultrasound/segmentation/sam/plot.py
import numpy as np
import matplotlib.pyplot as plt
import pickle

with open('sam_data.pkl', 'rb') as handle:
    data = pickle.load(handle)

for label in data:
    label_data = data[label]
    iou = []
    x_data = label_data["data"]
    for x in x_data:
        iou.append(x["iou"])
    iou = np.array(iou)
    bins = np.arange(0.0, 1.0+0.1, 0.1) # fixed number of bins
    # plt.xlim([min(data)-5, max(data)+5])
    plt.figure()
    plt.hist(iou, bins=bins, alpha=0.5)
    plt.title(f'{label}')
    plt.xlabel('iou')
    plt.ylabel('number of images')
    # plt.show()
    plt.savefig(f'{label}.png')
    plt.close()
#+end_src
** Results
*** Base model
#+attr_latex: :width 400px :height 100px
[[./img/vit_b.png]]
*** Large model
#+attr_latex: :width 400px :height 100px
[[./img/vit_l.png]]


* [[id:fc2fbd95-72de-4a25-9cb6-f491b48c29e1][Fine-tune SAM]] 
* [[id:b9cdac99-0341-47a9-bf7a-59c1b6c87234][AutoSAM]] 
