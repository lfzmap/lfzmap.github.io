:PROPERTIES:
:ID:       c2ef2104-39a5-4e0b-a07a-425de90b641f
:END:
#+title: SAM

* Paper
- https://arxiv.org/pdf/2304.02643.pdf
* Basics
Large language models pre-trained on large datasets (web-scale) are known as *Foundation models*. These models show very good performance on unseen dataset ie Zero-shot learning and Few-shot learning. Further they can also be used for various tasks or even out-of-distribution dataset. This is enabled by Prompt engineering, using text to prompt the model to give a valid response depending on the task.

CLIP and ALIGN are examples of foundation model that tries to align text and image modalities together. Goal of SAM is /to create a foundation model for image segmentation/.

Following components are crucial in creating a foundation model:
1. Which task provides better zero shot generalization?
2. What would be the corresponding model architecture?
3. What data is required?

Ans 1: Promptable segmentation task
Ans 2: Model with a flexible prompt encoder and mask generator
Ans 3: A data engine using model itself to help the annotations

* Segment Anything Task
In NLP task for pretraining foundation model is /next token prediction/. In segmentation a prompt would be set of points, bounding box, masks, free form text etc. A valid output would be a valid mask even if we give vague prompts for multiple objects, there should be a minimum one reasonable mask. 

[[./img/samtask.png]]

Aim of SAM is to be adaptable for other segmentation tasks, like:
1. Semantic segmentation
   pixelwise class label eg: cars-red, background-green
2. Instance segmentation
   object detection+segmentation. eg: car_1, car_2
3. Panoptic segmentation
   each pixel has two identifiers: class label and instance id. ie semantic+instance
4. Interactive segmentation
5. Edge detection
6. Foreground segmentation
7. Object proposal segmentation

SAM can act as a component in a larger system addressing different segmentation task.

* Segment Anything Model

Constraints:
1. Support Flexible prompts
2. Generate masks in amortized real-time
3. Ambiguity aware

   A 3 component design
[[./img/sammodel.png]]

Ans 1: A prompt encoder enables handling multiple type of prompts
Ans 2: We can use a single image embedding with multiple prompts giving us a 50 ms inference time for prompt encoder+mask decoder.
Ans 3: A single prompt and embedding will generate multiple mask to handle the ambiguity.

[[./img/sammodel2.png]]

** Image Encoder
It can be any network that output $C \times H \times W$. SAM uses an MAE pretrained Vision transformer.

TODO:
Attention
Transformer
Vision transformer
MAE ViT

* Installation
#+begin_src sh
$pip install git+https://github.com/facebookresearch/segment-anything.git
$pip install opencv-python pycocotools matplotlib onnxruntime onnx
#+end_src
* Download weights
- Download model checkpoints from [[https://github.com/facebookresearch/segment-anything?tab=readme-ov-file#model-checkpoints][here]].
  - h=Huge
  - l=Largw
  - B=base
* Usage
** Necessary imports 
#+begin_src python
import os
import pickle
import time
import cv2
import torch
import matplotlib.pyplot as plt
from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
import numpy as np

#+end_src
** Fix opencv-pyqt5 error
#+begin_src python
os.environ.pop("QT_QPA_PLATFORM_PLUGIN_PATH")

#+end_src
** Create overlay function
#+begin_src python
if not torch.cuda.is_available():
    raise Exception("CUDA not available!")
else:
    print("CUDA available")
    device = torch.device('cuda')

def show_anns(anns):
    if len(anns) == 0:
        return
    # sort as per area
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)
    # create zero matrix with alpha channel=1
    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:,:,3] = 0
    for ann in sorted_anns:
        m = ann['segmentation'] # get individual binary mask
        # create random color with alpha=0.35
        color_mask = np.concatenate([np.random.random(3), [0.35]])
        # replace True with color
        img[m] = color_mask
    ax.imshow(img)
    return img

#+end_src
** Setup SAM model
#+begin_src python
# sam model setup
sam = sam_model_registry["vit_l"](checkpoint="ckpts/sam_vit_l_0b3195.pth")
sam = sam.to(device=device)
mask_generator = SamAutomaticMaskGenerator(sam)

root = "/home/lfz/projects/ultrasound/datasets/mediscan-seg"
os.makedirs("./results", exist_ok=True)

#+end_src
** Generate and save SAM results
#+begin_src python
data = {}
for label in os.listdir(root):
    sub_root = os.path.join(root,label,"images")
    label_data = {}
    x_dicts = []
    label_data["num"] = len(os.listdir(sub_root))
    imgs = os.listdir(sub_root)
    for x in imgs: #os.listdir(sub_root):
        x_dict = {}
        x_dict["name"] = x
        img_path = os.path.join(sub_root,x)
        # read input image
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # plt.figure()
        plt.figure(figsize=(20,20))
        plt.imshow(img)
        plt.axis('off')
        # mask generation using sam
        start_ts = time.time()
        masks = mask_generator.generate(img)
        end_ts = time.time()
        t = (end_ts-start_ts)
        true_mask = cv2.imread(img_path.replace("images","masks"))
        true_mask = cv2.cvtColor(true_mask, cv2.COLOR_BGR2GRAY)
        true_mask[true_mask>=200]=255.0
        true_mask[true_mask<200]=0.0
        true_mask = true_mask.astype(bool)
        iou = []
        for ann in masks:
            m = ann['segmentation']
            intersection = np.logical_and(m, true_mask)
            union = np.logical_or(m, true_mask)
            iou_score = np.sum(intersection) / np.sum(union)
            iou.append(iou_score)
        iou = np.array(iou)
        iou_best = np.max(iou)
        idx = np.argmax(iou)
        best_mask = masks[idx]['segmentation']
        
        masks_overlay = show_anns(masks)
        plt.savefig(os.path.join("results", x), bbox_inches='tight')
        plt.close()
        print(f'{label}/{x} : iou = {iou_best:.3f} time = {t:.3f}')
        x_dict["infer_time"] = t
        x_dict["iou"] = iou_best
        x_dict["mask"] = best_mask
        x_dicts.append(x_dict)
    label_data["data"] = x_dicts
    data[label] = label_data

with open('sam_data.pkl', 'wb') as handle:
    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)
#+end_src

* FPUS23
#+begin_src python :tangle ~/projects/ultrasound/fpus23/fpus23.py :mkdirp yes :ignore
import os
import numpy as np
import cv2
from segment_anything import sam_model_registry, SamPredictor
from xml.etree import ElementTree as ET
import matplotlib.pyplot as plt
from PIL import Image

os.environ.pop("QT_QPA_PLATFORM_PLUGIN_PATH")

def show_box(box, ax):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)) 

def show_mask(mask, ax, random_color=False):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

model_type="vit_b"
sam_ckpt="/home/lfz/projects/ultrasound/weights/sam_vit_b.pth"
sam = sam_model_registry[model_type](checkpoint=sam_ckpt)
predictor = SamPredictor(sam)
print("Pretrained SAM loaded!")

ano_path = "/home/lfz/projects/ultrasound/data/fpus23/Dataset/boxes/annotation"
img_path = "/home/lfz/projects/ultrasound/data/fpus23/Dataset/four_poses"

bbox = []
X = []
Y = []
la = []
LA = []

# Scan all xml files for labels and image names
for obj in os.listdir(ano_path):
    file_name = os.path.join(ano_path, obj, 'annotations.xml')
    dom = ET.parse(file_name)

    names = dom.findall('image')
    for n in names:
        bbox = []
        la = []
        name = n.attrib.get('name')
        lab = n.findall('box')
        
        if not (lab == []):
            for l in lab:
                xtl = l.attrib.get('xtl')
                ytl = l.attrib.get('ytl')
                xbr = l.attrib.get('xbr')
                ybr = l.attrib.get('ybr')
                label = l.attrib.get('label')
                box = [xtl, ytl, xbr, ybr]

                if label == 'head':
                    la.append(1)
                elif label == 'abdomen':
                    la.append(2)
                elif label == 'arm':
                    la.append(3)
                elif label == 'legs':
                    la.append(4)

                bbox.append(box)
            x = os.path.join(img_path, obj,name)
            Y.append(bbox)
            X.append(x)
            LA.append(la)

print(len(X), len(Y), len(LA))
for idx,img_name in enumerate(X):
    print(img_name)
    img = cv2.imread(img_name)
    img_res = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)#.astype(np.float32)
    predictor.set_image(img)
    # diving by 255
    # img_res /= 255.0

    # cv2 image gives size as height x width
    wt = img.shape[1]
    ht = img.shape[0]

    labels = LA[idx]
    boxes = Y[idx]
    boxes = np.array(boxes).astype(float)

    bmp_path = img_name.replace("four_poses", "masks")
    bmp_path = bmp_path.replace("png", "bmp")
    # bmp_folder = "/".join(bmp_path.split("/")[:-1])
    bmp_folder = bmp_path[:-4]
    os.makedirs(bmp_folder, exist_ok=True)

    overlay_path = img_name.replace("four_poses", "overlay")
    overlay_folder = "/".join(overlay_path.split("/")[:-1])
    os.makedirs(overlay_folder, exist_ok=True)
    plt.imshow(img)
    for i,box in enumerate(boxes):
        masks, _, _ = predictor.predict(
            point_coords=None,
            point_labels=None,
            box=box[None, :],
            multimask_output=False,
        )
        mask = masks[0]
        la = labels[i]
        show_box(box, plt.gca())

        image_bmp = Image.fromarray(mask)
        bmp_path = os.path.join(bmp_folder,f"{i}_{la}.bmp")
        image_bmp.save(bmp_path, format='BMP')

    show_mask(mask, plt.gca())
    plt.savefig(overlay_path, bbox_inches='tight')
    plt.close()
    # break
#+end_src

* [[id:fc2fbd95-72de-4a25-9cb6-f491b48c29e1][Fine-tune SAM]] 
* [[id:b9cdac99-0341-47a9-bf7a-59c1b6c87234][AutoSAM]] 
* [[id:652855c4-c6cb-476c-a8fd-39540e3e0c59][MobileSAM]] 
