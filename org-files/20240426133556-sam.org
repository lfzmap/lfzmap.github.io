:PROPERTIES:
:ID:       c2ef2104-39a5-4e0b-a07a-425de90b641f
:END:
#+title: SAM

* SAM-1
** Paper
- https://arxiv.org/pdf/2304.02643.pdf
** Basics
Large language models pre-trained on large datasets (web-scale) are known as *Foundation models*. These models show very good performance on unseen dataset ie Zero-shot learning and Few-shot learning. Further they can also be used for various tasks or even out-of-distribution dataset. This is enabled by Prompt engineering, using text to prompt the model to give a valid response depending on the task.

CLIP and ALIGN are examples of foundation model that tries to align text and image modalities together. Goal of SAM is /to create a foundation model for image segmentation/.

Following components are crucial in creating a foundation model:
1. Which task provides better zero shot generalization?
2. What would be the corresponding model architecture?
3. What data is required?

Ans 1: Promptable segmentation task
Ans 2: Model with a flexible prompt encoder and mask generator
Ans 3: A data engine using model itself to help the annotations

** Segment Anything Task
In NLP task for pretraining foundation model is /next token prediction/. In segmentation a prompt would be set of points, bounding box, masks, free form text etc. A valid output would be a valid mask even if we give vague prompts for multiple objects, there should be a minimum one reasonable mask. 

[[./img/samtask.png]]

Aim of SAM is to be adaptable for other segmentation tasks, like:
1. Semantic segmentation
   pixelwise class label eg: cars-red, background-green
2. Instance segmentation
   object detection+segmentation. eg: car_1, car_2
3. Panoptic segmentation
   each pixel has two identifiers: class label and instance id. ie semantic+instance
4. Interactive segmentation
5. Edge detection
6. Foreground segmentation
7. Object proposal segmentation

SAM can act as a component in a larger system addressing different segmentation task.

** Segment Anything Model

Constraints:
1. Support Flexible prompts
2. Generate masks in amortized real-time
3. Ambiguity aware

   A 3 component design
[[./img/sammodel.png]]

Ans 1: A prompt encoder enables handling multiple type of prompts
Ans 2: We can use a single image embedding with multiple prompts giving us a 50 ms inference time for prompt encoder+mask decoder.
Ans 3: A single prompt and embedding will generate multiple mask to handle the ambiguity.

[[./img/sammodel2.png]]

*** Image Encoder
It can be any network that output $C \times H \times W$. SAM uses an MAE pretrained Vision transformer.

TODO:
Attention
Transformer
Vision transformer
MAE ViT

** Installation
#+begin_src sh
$pip install git+https://github.com/facebookresearch/segment-anything.git
$pip install opencv-python pycocotools matplotlib onnxruntime onnx
#+end_src
** Download weights
- Download model checkpoints from [[https://github.com/facebookresearch/segment-anything?tab=readme-ov-file#model-checkpoints][here]].
  - h=Huge
  - l=Largw
  - B=base
** Usage
*** Necessary imports 
#+begin_src python
import os
import pickle
import time
import cv2
import torch
import matplotlib.pyplot as plt
from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
import numpy as np

#+end_src
*** Fix opencv-pyqt5 error
#+begin_src python
os.environ.pop("QT_QPA_PLATFORM_PLUGIN_PATH")

#+end_src
*** Create overlay function
#+begin_src python
if not torch.cuda.is_available():
    raise Exception("CUDA not available!")
else:
    print("CUDA available")
    device = torch.device('cuda')

def show_anns(anns):
    if len(anns) == 0:
        return
    # sort as per area
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)
    # create zero matrix with alpha channel=1
    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:,:,3] = 0
    for ann in sorted_anns:
        m = ann['segmentation'] # get individual binary mask
        # create random color with alpha=0.35
        color_mask = np.concatenate([np.random.random(3), [0.35]])
        # replace True with color
        img[m] = color_mask
    ax.imshow(img)
    return img

#+end_src
*** Setup SAM model
#+begin_src python
# sam model setup
sam = sam_model_registry["vit_l"](checkpoint="ckpts/sam_vit_l_0b3195.pth")
sam = sam.to(device=device)
mask_generator = SamAutomaticMaskGenerator(sam)

root = "/home/lfz/projects/ultrasound/datasets/mediscan-seg"
os.makedirs("./results", exist_ok=True)

#+end_src
*** Generate and save SAM results
#+begin_src python
data = {}
for label in os.listdir(root):
    sub_root = os.path.join(root,label,"images")
    label_data = {}
    x_dicts = []
    label_data["num"] = len(os.listdir(sub_root))
    imgs = os.listdir(sub_root)
    for x in imgs: #os.listdir(sub_root):
        x_dict = {}
        x_dict["name"] = x
        img_path = os.path.join(sub_root,x)
        # read input image
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        # plt.figure()
        plt.figure(figsize=(20,20))
        plt.imshow(img)
        plt.axis('off')
        # mask generation using sam
        start_ts = time.time()
        masks = mask_generator.generate(img)
        end_ts = time.time()
        t = (end_ts-start_ts)
        true_mask = cv2.imread(img_path.replace("images","masks"))
        true_mask = cv2.cvtColor(true_mask, cv2.COLOR_BGR2GRAY)
        true_mask[true_mask>=200]=255.0
        true_mask[true_mask<200]=0.0
        true_mask = true_mask.astype(bool)
        iou = []
        for ann in masks:
            m = ann['segmentation']
            intersection = np.logical_and(m, true_mask)
            union = np.logical_or(m, true_mask)
            iou_score = np.sum(intersection) / np.sum(union)
            iou.append(iou_score)
        iou = np.array(iou)
        iou_best = np.max(iou)
        idx = np.argmax(iou)
        best_mask = masks[idx]['segmentation']
        
        masks_overlay = show_anns(masks)
        plt.savefig(os.path.join("results", x), bbox_inches='tight')
        plt.close()
        print(f'{label}/{x} : iou = {iou_best:.3f} time = {t:.3f}')
        x_dict["infer_time"] = t
        x_dict["iou"] = iou_best
        x_dict["mask"] = best_mask
        x_dicts.append(x_dict)
    label_data["data"] = x_dicts
    data[label] = label_data

with open('sam_data.pkl', 'wb') as handle:
    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)
#+end_src
** Check attention 
#+begin_src python :tangle ~/projects/ultrasound/check.py
import torch
import torch.nn as nn
from segment_anything import sam_model_registry

class LayerNorm2d(nn.Module):
    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:
        super().__init__()
        self.weight = nn.Parameter(torch.ones(num_channels))
        self.bias = nn.Parameter(torch.zeros(num_channels))
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        u = x.mean(1, keepdim=True)
        s = (x - u).pow(2).mean(1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.eps)
        x = self.weight[:, None, None] * x + self.bias[:, None, None]
        return x

# [2, 64, 64, 768]
class Sam(nn.Module):
    def __init__(self, model_type, sam_ckpt):
        super(Sam, self).__init__()
        # a dict to store the intermediate activations
        self.activation = {}

        self.model_type = model_type 
        self.sam_ckpt = sam_ckpt
        self.sam = sam_model_registry[model_type](checkpoint=sam_ckpt)
        self.sam.eval()
        self.hooks = []

        for i in range(12):
            self.hooks.append(self.sam.image_encoder.blocks[i].register_forward_hook(self.getActivation("act"+str(i))))
        print(self.sam.image_encoder)
        print("Pretrained SAM loaded!")

    def getActivation(self,name):
        # the hook signature
        def hook(model, input, output):
            self.activation[name] = output.detach()
        return hook

    def encode_img(self, x, freeze=True):
        x = torch.stack([self.sam.preprocess(img) for img in x], dim=0)
        img_embed = self.sam.image_encoder(x) 
        return img_embed # (B, 256,64,64)

    def forward(self, x):
        with torch.no_grad():
            x = self.encode_img(x)
        for i in range(12):
            print(self.activation["act"+str(i)].size())
            self.hooks[i].remove()
        # x = self.neck(self.activation["act11"].permute(0, 3, 1, 2))
        return x

if __name__ == "__main__":

    model_type="vit_b"
    sam_ckpt="/home/lfz/projects/ultrasound/weights/sam_vit_b.pth"
    model = Sam(model_type, sam_ckpt)
    
    x = torch.randn(2, 3, 1024, 1024)
    o = model(x)
    print(o.size())
#+end_src


** [[id:fc2fbd95-72de-4a25-9cb6-f491b48c29e1][Fine-tune SAM]] 
** [[id:b9cdac99-0341-47a9-bf7a-59c1b6c87234][AutoSAM]] 
** [[id:652855c4-c6cb-476c-a8fd-39540e3e0c59][MobileSAM]] 
* SAM 2
Paper [[https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/][Link]]
** Introduction
An image is only a static snapshot of the real world in which visual segments can exhibit complex motion, and with the rapid growth of multimedia content, a significant portion is now recorded with a temporal dimension, particularly in video data.
*** Challenges regarding video
- Appearence change due to motion, deformation, occlusion, lighting change
- Low quality due to camera motion, blur, resolution
- Processing of large number of frames

An image is considered as a single frame video.
Not restricted to objects of specific categories, but instead targeted to provide training data for segmenting any object with a valid boundary, including parts and subparts.
** Model
Unlike SAM, the frame embedding used by the SAM 2 decoder is not directly from an image encoder and is instead conditioned on memories of past predictions and prompted frames. It is possible for prompted frames to also come “from the future” relative to the current frame. Memories of frames are created by the memory encoder based on the current prediction and placed in a memory bank for use in subsequent frames. The memory attention operation takes the per-frame embedding from the image encoder and conditions it on the memory bank to produce an embedding that is then passed to the mask decoder.

